{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ULM baseline experiments\n",
    "\n",
    "My first attempts at using tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sys import stdin\n",
    "from collections import defaultdict\n",
    "from scipy.io import mmread\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import re\n",
    "import datetime as dt\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from os import path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_path = '/mnt/store/hlt/Language/Hungarian/Crawl/Web2/ana/xaa.tagged'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class ResearchDiary:\n",
    "    diary_file = \"baseline_experiment_results.tsv\"\n",
    "        \n",
    "    @staticmethod\n",
    "    def load_or_create_dataframe():\n",
    "        if path.exists(ResearchDiary.diary_file):\n",
    "            result_diary = pd.read_table(ResearchDiary.diary_file)\n",
    "        else:\n",
    "            result_diary = pd.DataFrame(columns=[\n",
    "                                            'train_accuracy',\n",
    "                                            'test_accuracy',\n",
    "                                            'timestamp',\n",
    "                                            'data_path',\n",
    "                                            'theoretical_max',\n",
    "                                            'architecture',\n",
    "                                            'min_sample_per_class',\n",
    "                                            'max_sample_per_class',\n",
    "                                            'sample_count',\n",
    "                                            'feature_count',\n",
    "                                            'max_lines',\n",
    "                                            'N',\n",
    "                                            'last_char',\n",
    "                                            'full_tag',\n",
    "                                            'tag_filter',\n",
    "                                            'include_smaller_ngrams',\n",
    "                                            'use_padding',\n",
    "                                            'epochs',\n",
    "                                            'layers',\n",
    "                                            'activation',\n",
    "                                            'batch_size',\n",
    "                                            'optimizer',\n",
    "                                            'optimizer_kwargs',\n",
    "                                            'gpu_memory_fracion',\n",
    "                                            'running_time'\n",
    "                                        ])\n",
    "        return result_diary\n",
    "        \n",
    "    def __init__(self):\n",
    "        self.df = ResearchDiary.load_or_create_dataframe()\n",
    "        \n",
    "    def __enter__(self):\n",
    "        return self\n",
    "    \n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        self.df.to_csv(ResearchDiary.diary_file, sep='\\t', index=False)\n",
    "    \n",
    "    def add_experiment(self, data, create_cols_if_new=True):\n",
    "        new_index = len(self.df)\n",
    "        if create_cols_if_new:\n",
    "            for new_col in set(data.keys()) - set(self.df.columns):\n",
    "                self.df[new_col] = None\n",
    "        else:\n",
    "            missing_cols = set(data.keys()) - set(self.df.columns)\n",
    "            for c in missing_cols:\n",
    "                del data[c]\n",
    "        self.df.loc[new_index] = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import defaultdict\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "\n",
    "class Featurizer:\n",
    "    \n",
    "    class InvalidTag(Exception):\n",
    "        pass\n",
    "    \n",
    "    tag_re = re.compile(r'^[A-Z]+')\n",
    "    \n",
    "    def __init__(self, last_char, N, full_tag=False, use_padding=True, use_word_as_feature=False, tag_filter=None,\n",
    "                 include_smaller_ngrams=False, **kwargs):\n",
    "        self.last_char = last_char\n",
    "        self.N = N\n",
    "        self.full_tag = full_tag\n",
    "        self.use_padding = use_padding\n",
    "        self.use_word_as_feature = use_word_as_feature\n",
    "        self.include_smaller_ngrams = include_smaller_ngrams\n",
    "        if tag_filter is not None:\n",
    "            self.tag_filter = set(tag_filter)\n",
    "        else:\n",
    "            self.tag_filter = None\n",
    "        self.raw_input = []\n",
    "        \n",
    "    def featurize(self, data_path, encoding='utf-8', min_sample_per_class=0, max_sample_per_class=0, max_lines=0,\n",
    "                 **kwargs):\n",
    "        X = []\n",
    "        y = []\n",
    "        self.min_sample_per_class = min_sample_per_class\n",
    "        self.max_sample_per_class = max_sample_per_class\n",
    "        self.max_lines = max_lines\n",
    "        sample_cnt = defaultdict(int)\n",
    "        line_cnt = 0\n",
    "        with open(data_path, encoding=encoding) as f:\n",
    "            for line in f:\n",
    "                line_cnt += 1\n",
    "                if max_lines > 0 and line_cnt > max_lines:\n",
    "                    break\n",
    "                try:\n",
    "                    word, tag = self.extract_word_and_tag(line)\n",
    "                except Featurizer.InvalidTag:\n",
    "                    continue\n",
    "                if not word.strip() or not tag.strip():\n",
    "                    continue\n",
    "                if self.tag_filter is not None and tag not in self.tag_filter:\n",
    "                    continue\n",
    "                if max_sample_per_class > 0 and sample_cnt[tag] >= max_sample_per_class:\n",
    "                    continue\n",
    "                if all(v >= max_sample_per_class for v in sample_cnt.values()) and len(sample_cnt) > 1:\n",
    "                    break\n",
    "                self.__featurize_and_store_sample(word, tag, X, y)\n",
    "                self.raw_input.append((word, tag))\n",
    "                sample_cnt[tag] += 1\n",
    "        return self.create_feature_matrix(X, y)\n",
    "                \n",
    "    def create_feature_matrix(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.X_mtx = self.__get_or_create_vectorizer('X_vectorizer', X)\n",
    "        self.y_vec = self.__get_or_create_vectorizer('y_vectorizer', y)\n",
    "        return self.X_mtx, self.y_vec\n",
    "        \n",
    "    def __get_or_create_vectorizer(self, name, data):\n",
    "        if not hasattr(self, name):\n",
    "            dv = DictVectorizer()\n",
    "            v = dv.fit_transform(data)\n",
    "            setattr(self, name, dv)\n",
    "        return v\n",
    "    \n",
    "    def extract_word_and_tag(self, line):\n",
    "        fd = line.strip().split('\\t')\n",
    "        word = fd[0]\n",
    "        tag = fd[-1].split('/')[-1]\n",
    "        if self.full_tag is False:\n",
    "            try:\n",
    "                tag = Featurizer.tag_re.match(tag).group(0)\n",
    "            except AttributeError:\n",
    "                raise Featurizer.InvalidTag()\n",
    "        return word, tag\n",
    "    \n",
    "    def __featurize_and_store_sample(self, word, tag, X, y):\n",
    "        if self.use_word_as_feature:\n",
    "            f = {'word': word}\n",
    "        else:\n",
    "            f = self.__featurize_ngram(word)\n",
    "        X.append(f)\n",
    "        y.append({'class': tag})\n",
    "            \n",
    "    def __featurize_ngram(self, word):\n",
    "        feats = {}\n",
    "        if self.last_char > 0:\n",
    "            word = word[-self.last_char:]\n",
    "        if self.include_smaller_ngrams:\n",
    "            for n in range(1, self.N+1):\n",
    "                feats.update(Featurizer.extract_ngrams(word, n, self.use_padding))\n",
    "        else:\n",
    "            feats.update(Featurizer.extract_ngrams(word, self.N, self.use_padding))\n",
    "        return feats\n",
    "    \n",
    "    @staticmethod\n",
    "    def extract_ngrams(text, N, padding=False):\n",
    "        if padding is True:\n",
    "            text = '{0}{1}{0}'.format(\" \" * (N-1), text)\n",
    "        feats = {}\n",
    "        for i in range(len(text)-N+1):\n",
    "            feats['{0}_{1}'.format(N, len(text) - i)] = text[i:i+N]\n",
    "        return feats\n",
    "            \n",
    "    def get_theoretical_max(self):\n",
    "        samples = defaultdict(lambda: defaultdict(int))\n",
    "        for i in range(len(self.X)):\n",
    "            xi = self.X[i]\n",
    "            yi = self.y[i]\n",
    "            f_str = ','.join('{}:{}'.format(feat, val) for feat, val in sorted(xi.items()))\n",
    "            samples[f_str][yi['class']] += 1\n",
    "        return sum(max(v.values()) for v in samples.values()) / len(self.X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "from scipy.sparse import issparse\n",
    "\n",
    "\n",
    "class FFNN:\n",
    "    \n",
    "    def __init__(self, n_feature, n_class, layers, batch_size=0, epochs=5000, verbose=False,\n",
    "                 activation=tf.sigmoid, gpu_memory_fraction=0.5,\n",
    "                 optimizer=\"GradientDescentOptimizer\", optimizer_kwargs={}):\n",
    "        self.n_feature = n_feature\n",
    "        self.n_class = n_class\n",
    "        self.shape = layers\n",
    "        self.batch_size = batch_size\n",
    "        self.n_input = tf.placeholder(tf.float32, shape=[None, n_feature],\n",
    "                         name=\"n_input\")\n",
    "        self.n_output = tf.placeholder(tf.float32, shape=[None, n_class],\n",
    "                          name=\"n_output\")\n",
    "        self.bias = []\n",
    "        self.W = []\n",
    "        self.hidden = []\n",
    "        self.activation = activation\n",
    "        self.create_input_layer()\n",
    "        self.create_hidden_layers()\n",
    "        self.create_output_layer()\n",
    "        self.cost = tf.reduce_mean(tf.square(self.n_output - self.output))\n",
    "        self.optimizer_kwargs = optimizer_kwargs\n",
    "        self.optimizer = getattr(tf.train, optimizer)(**optimizer_kwargs)\n",
    "        self.train = self.optimizer.minimize(self.cost)\n",
    "        self.init = tf.initialize_all_variables()\n",
    "        self.gpu_memory_fraction = gpu_memory_fraction\n",
    "        gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=gpu_memory_fraction)\n",
    "        self.sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))\n",
    "        self.sess.run(self.init)\n",
    "        self.verbose = verbose\n",
    "        self.epochs = epochs\n",
    "        \n",
    "    def create_input_layer(self):\n",
    "        self.bias.append(tf.Variable(tf.random_normal([self.shape[0]]), name=\"bias0\"))\n",
    "        self.W.append(tf.Variable(tf.random_normal([self.n_feature, self.shape[0]]), name=\"weights0\"))\n",
    "        self.hidden.append(self.activation(tf.matmul(self.n_input, self.W[0]) + self.bias[0]))\n",
    "        \n",
    "    def create_hidden_layers(self):\n",
    "        for i in range(1, len(self.shape)):\n",
    "            self.bias.append(tf.Variable(tf.random_normal([self.shape[i]]), name=\"bias{}\".format(i)))\n",
    "            self.W.append(tf.Variable(tf.random_normal([self.shape[i-1], self.shape[i]]), name=\"weights0\"))\n",
    "            self.hidden.append(self.activation(tf.matmul(self.hidden[-1], self.W[-1]) + self.bias[-1]))\n",
    "            \n",
    "    def create_output_layer(self):\n",
    "        self.bias.append(tf.Variable(tf.random_normal([self.n_class]), name=\"bias{}\".format(len(self.shape)-1)))\n",
    "        self.W.append(tf.Variable(tf.random_normal([self.shape[-1], self.n_class]), name=\"weights{}\".format(len(self.shape)-1)))\n",
    "        self.output = self.activation(tf.matmul(self.hidden[-1], self.W[-1]) + self.bias[-1])\n",
    "        \n",
    "    def dotrain(self, X_train, y_train):\n",
    "        X_train = FFNN.convert_sparse_if_needed(X_train)\n",
    "        y_train = FFNN.convert_sparse_if_needed(y_train)\n",
    "        cvalues = []\n",
    "        step = self.epochs // 10\n",
    "        step = 1 if step < 1 else step\n",
    "        cnt = 0\n",
    "        for epoch in range(0, self.epochs):\n",
    "            if self.batch_size > 0:\n",
    "                X_batch, y_batch = self.get_minibatch(X_train, y_train)\n",
    "            else:\n",
    "                X_batch = X_train\n",
    "                y_batch = y_train\n",
    "            cvalues.append(self.sess.run([self.train, self.cost] + self.W + self.bias,\n",
    "                    feed_dict={self.n_input: X_batch, self.n_output: y_batch}))\n",
    "            cnt += 1\n",
    "            if cnt % step == 0 and self.verbose is True:\n",
    "                print('{0} epochs, cvalue: {1}'.format(epoch+1, cvalues[-1][1]))\n",
    "        self.correct_prediction = tf.equal(tf.argmax(self.n_output,1), tf.argmax(self.output,1))\n",
    "        return cvalues\n",
    "        \n",
    "    def get_minibatch(self, X, y):\n",
    "        batch_index = np.random.choice(np.arange(0, X.shape[0]), self.batch_size)\n",
    "        return X[batch_index], y[batch_index]\n",
    "    \n",
    "    def dotest(self, X_test, y_test):\n",
    "        X_test = FFNN.convert_sparse_if_needed(X_test)\n",
    "        y_test = FFNN.convert_sparse_if_needed(y_test)\n",
    "        accuracy = tf.reduce_mean(tf.cast(self.correct_prediction, tf.float32))\n",
    "        prediction = self.correct_prediction.eval(session=self.sess,\n",
    "                                                  feed_dict={self.n_input: X_test, self.n_output: y_test})\n",
    "        return accuracy.eval(session=self.sess, feed_dict={self.n_input: X_test, self.n_output: y_test}), prediction\n",
    "        \n",
    "    @staticmethod\n",
    "    def convert_sparse_if_needed(mtx):\n",
    "        if issparse(mtx):\n",
    "            mtx = mtx.todense()\n",
    "        return mtx\n",
    "    \n",
    "\n",
    "class Experiment:\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        if 'global' not in config:\n",
    "            glob_config = {}\n",
    "        else:\n",
    "            glob_config = config['global']\n",
    "        self.featurizer = Featurizer(**config['featurizer'])\n",
    "        X, y = self.featurizer.featurize(**config['featurizer'])\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.test_size = glob_config.get('test_size', .1)\n",
    "        verbose = glob_config.get('verbose', False)\n",
    "        self.ffnn = FFNN(X.shape[1], y.shape[1], verbose=verbose, **config['ffnn'])\n",
    "        \n",
    "    def run(self):\n",
    "        self.train_mask = np.random.random(size=self.X.shape[0]) > self.test_size\n",
    "        X_train = self.X[self.train_mask]\n",
    "        X_test = self.X[np.invert(self.train_mask)]\n",
    "        y_train = self.y[self.train_mask]\n",
    "        y_test = self.y[np.invert(self.train_mask)]\n",
    "        cvalues = self.ffnn.dotrain(X_train, y_train)\n",
    "        train_acc, train_pred = self.ffnn.dotest(X_train, y_train)\n",
    "        test_acc, test_pred = self.ffnn.dotest(X_test, y_test)\n",
    "        self.test_pred = test_pred\n",
    "        self.train_acc = train_acc\n",
    "        self.test_acc = test_acc\n",
    "        return train_acc, train_pred, test_acc, test_pred\n",
    "    \n",
    "    def run_decision_tree(self):\n",
    "        self.clf = DecisionTreeClassifier()\n",
    "        return cross_val_score(self.clf, self.X.toarray(), self.y.toarray(), cv=10)\n",
    "        \n",
    "    def get_test_errors(self):\n",
    "        test_samples = [p[1] for p in filter(lambda x: not self.train_mask[x[0]], enumerate(self.featurizer.raw_input))]\n",
    "        errors = []\n",
    "        for i, s in enumerate(test_samples):\n",
    "            if not self.test_pred[i]:\n",
    "                errors.append(s)\n",
    "        return errors \n",
    "    \n",
    "    def add_results_to_diary(self, diary):\n",
    "        d = {\n",
    "            'timestamp': dt.datetime.now(),\n",
    "            'data_path': data_path,\n",
    "            'last_char': self.featurizer.last_char,\n",
    "            'N': self.featurizer.N,\n",
    "            'full_tag': self.featurizer.full_tag,\n",
    "            'use_padding': self.featurizer.use_padding,\n",
    "            'include_smaller_ngrams': self.featurizer.include_smaller_ngrams,\n",
    "            'tag_filter': self.featurizer.tag_filter,\n",
    "            'min_sample_per_class': self.featurizer.min_sample_per_class,\n",
    "            'max_sample_per_class': self.featurizer.max_sample_per_class,\n",
    "            'max_lines': self.featurizer.max_lines,\n",
    "            'sample_count': self.featurizer.X_mtx.shape[0],\n",
    "            'feature_count': self.featurizer.X_mtx.shape[1],\n",
    "            'architecture': 'FFNN',\n",
    "            'layers': self.ffnn.shape,\n",
    "            'batch_size': self.ffnn.batch_size,\n",
    "            'epochs': self.ffnn.epochs,\n",
    "            'activation': self.ffnn.activation.__name__,\n",
    "            'gpu_memory_fracion': self.ffnn.gpu_memory_fraction,\n",
    "            'optimizer': self.ffnn.optimizer.__class__,\n",
    "            'optimizer_kwargs': self.ffnn.optimizer_kwargs,\n",
    "            'train_accuracy': self.train_acc,\n",
    "            'test_accuracy': self.test_acc,\n",
    "            'theoretical_max': self.featurizer.get_theoretical_max(),\n",
    "            'running_time': self.running_time,\n",
    "        }\n",
    "        diary.add_experiment(d)\n",
    "    \n",
    "    def run_and_save(self):\n",
    "        start = dt.datetime.now()\n",
    "        train_acc, train_pred, test_acc, test_pred = self.run()\n",
    "        end = dt.datetime.now()\n",
    "        self.running_time = end - start\n",
    "        print(\"Training accuracy: {0}\\nTest accuracy: {1}\".format(train_acc, test_acc))\n",
    "        with ResearchDiary() as rd:\n",
    "            self.add_results_to_diary(rd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 epochs, cvalue: 0.353938490152359\n",
      "10 epochs, cvalue: 0.27243995666503906\n",
      "15 epochs, cvalue: 0.2680909037590027\n",
      "20 epochs, cvalue: 0.261005699634552\n",
      "25 epochs, cvalue: 0.25506535172462463\n",
      "30 epochs, cvalue: 0.25461551547050476\n",
      "35 epochs, cvalue: 0.25099408626556396\n",
      "40 epochs, cvalue: 0.2432461529970169\n",
      "45 epochs, cvalue: 0.24172766506671906\n",
      "50 epochs, cvalue: 0.23861713707447052\n",
      "Training accuracy: 0.6244697570800781\n",
      "Test accuracy: 0.5988483428955078\n",
      "CPU times: user 3.61 s, sys: 0 ns, total: 3.61 s\n",
      "Wall time: 3.47 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "e = Experiment({\n",
    "        'global': {\n",
    "            'verbose': True,\n",
    "        },\n",
    "        'featurizer': {\n",
    "            'last_char': 5,\n",
    "            'N': 1,\n",
    "            'tag_filter': (\"NOUN\", \"VERB\"),\n",
    "            'data_path': data_path,\n",
    "            'encoding': 'latin2',\n",
    "            'min_sample_per_class': 3,\n",
    "            'max_sample_per_class': 2500,\n",
    "            'max_lines': 2000000,\n",
    "        },\n",
    "        'ffnn': {\n",
    "            'layers': (40, 40, 40),\n",
    "            'optimizer': 'MomentumOptimizer',\n",
    "            'batch_size': 1000,\n",
    "            'optimizer_kwargs': {\n",
    "                'learning_rate': 1,\n",
    "                'momentum': .1,\n",
    "            },\n",
    "            'gpu_memory_fraction': 1,\n",
    "            'epochs': 50,\n",
    "        }\n",
    "})\n",
    "e.run_and_save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.918,  0.968,  0.954,  0.894,  0.898,  0.954,  0.94 ,  0.898,\n",
       "        0.926,  0.954])"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e.run_decision_tree()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 epochs, cvalue: 0.1127418652176857\n",
      "2000 epochs, cvalue: 0.0853806883096695\n",
      "3000 epochs, cvalue: 0.07170069962739944\n",
      "4000 epochs, cvalue: 0.06646432727575302\n",
      "5000 epochs, cvalue: 0.05410183593630791\n",
      "6000 epochs, cvalue: 0.058784421533346176\n",
      "7000 epochs, cvalue: 0.04342120513319969\n",
      "8000 epochs, cvalue: 0.05368638038635254\n",
      "9000 epochs, cvalue: 0.04481387510895729\n",
      "10000 epochs, cvalue: 0.04545537754893303\n",
      "Training accuracy: 0.9506864547729492\n",
      "Test accuracy: 0.9454238414764404\n",
      "CPU times: user 49.8 s, sys: 2.66 s, total: 52.4 s\n",
      "Wall time: 38.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "e = Experiment({\n",
    "        'global': {\n",
    "            'verbose': True,\n",
    "        },\n",
    "        'featurizer': {\n",
    "            'last_char': 5,\n",
    "            'N': 1,\n",
    "            'use_word_as_feature': False,\n",
    "            'tag_filter': (\"NOUN\", \"VERB\"),\n",
    "            'data_path': data_path,\n",
    "            'encoding': 'latin2',\n",
    "            'min_sample_per_class': 3,\n",
    "            'max_sample_per_class': 50000,\n",
    "            'max_lines': 2000000,\n",
    "        },\n",
    "        'ffnn': {\n",
    "            'layers': (40, 40),\n",
    "            'batch_size': 1000,\n",
    "            'optimizer': 'MomentumOptimizer',\n",
    "            'optimizer_kwargs': {\n",
    "                'learning_rate': 1,\n",
    "                'momentum': .1,\n",
    "            },\n",
    "            'gpu_memory_fraction': 1,\n",
    "            'epochs': 10000,\n",
    "        }\n",
    "})\n",
    "e.run_and_save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Viharjuk', 'NOUN'),\n",
       " ('értem', 'NOUN'),\n",
       " ('MacDonald', 'NOUN'),\n",
       " ('Monrningstar', 'NOUN'),\n",
       " ('heute', 'NOUN'),\n",
       " ('operál', 'VERB'),\n",
       " ('jelent', 'NOUN'),\n",
       " ('érzékel', 'VERB'),\n",
       " ('exhibicionista', 'NOUN'),\n",
       " ('alja', 'NOUN'),\n",
       " ('ér', 'VERB'),\n",
       " ('annak', 'NOUN'),\n",
       " ('univerzum', 'NOUN'),\n",
       " ('annak', 'NOUN'),\n",
       " ('annak', 'NOUN'),\n",
       " ('foglaltak', 'NOUN'),\n",
       " ('Oszlop', 'NOUN'),\n",
       " ('(hárs)méz', 'VERB'),\n",
       " ('sztirolhab', 'NOUN'),\n",
       " ('jogosult', 'NOUN')]"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "errors = e.get_test_errors()\n",
    "errors[:10]\n",
    "d = defaultdict(int)\n",
    "for i in errors:\n",
    "    d[i[1]] += 1\n",
    "d\n",
    "errors[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500 epochs, cvalue: 0.136236310005188\n",
      "1000 epochs, cvalue: 0.08828959614038467\n",
      "1500 epochs, cvalue: 0.06086685135960579\n",
      "2000 epochs, cvalue: 0.045603714883327484\n",
      "2500 epochs, cvalue: 0.03561263531446457\n",
      "3000 epochs, cvalue: 0.028374725952744484\n",
      "3500 epochs, cvalue: 0.022943317890167236\n",
      "4000 epochs, cvalue: 0.019268445670604706\n",
      "4500 epochs, cvalue: 0.01685773767530918\n",
      "5000 epochs, cvalue: 0.01519788708537817\n",
      "Training accuracy: 0.9856663942337036\n",
      "Test accuracy: 0.9121495485305786\n",
      "CPU times: user 2min 8s, sys: 3.06 s, total: 2min 11s\n",
      "Wall time: 2min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "e = Experiment({\n",
    "        'global': {\n",
    "            'verbose': True,\n",
    "        },\n",
    "        'featurizer': {\n",
    "            'last_char': 5,\n",
    "            'N': 2,\n",
    "            'use_word_as_feature': False,\n",
    "            'tag_filter': (\"NOUN\", \"VERB\"),\n",
    "            'data_path': data_path,\n",
    "            'encoding': 'latin2',\n",
    "            'min_sample_per_class': 3,\n",
    "            'max_sample_per_class': 2500,\n",
    "            'max_lines': 2000000,\n",
    "        },\n",
    "        'ffnn': {\n",
    "            'layers': (40, 40, 40),\n",
    "            'batch_size': 100,\n",
    "            'optimizer': 'MomentumOptimizer',\n",
    "            'optimizer_kwargs': {\n",
    "                'learning_rate': 1,\n",
    "                'momentum': .1,\n",
    "            },\n",
    "            'gpu_memory_fraction': 1,\n",
    "            'epochs': 5000,\n",
    "        }\n",
    "})\n",
    "e.run_and_save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500 epochs, cvalue: 0.29633498191833496\n",
      "1000 epochs, cvalue: 0.2696171998977661\n",
      "1500 epochs, cvalue: 0.24280379712581635\n",
      "2000 epochs, cvalue: 0.22048281133174896\n",
      "2500 epochs, cvalue: 0.20246997475624084\n",
      "3000 epochs, cvalue: 0.1871405839920044\n",
      "3500 epochs, cvalue: 0.17296195030212402\n",
      "4000 epochs, cvalue: 0.1607976108789444\n",
      "4500 epochs, cvalue: 0.15112482011318207\n",
      "5000 epochs, cvalue: 0.14164234697818756\n",
      "Training accuracy: 0.7874769568443298\n",
      "Test accuracy: 0.714035153388977\n",
      "CPU times: user 2min 57s, sys: 48.8 s, total: 3min 46s\n",
      "Wall time: 3min 46s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "e = Experiment({\n",
    "        'global': {\n",
    "            'verbose': True,\n",
    "        },\n",
    "        'featurizer': {\n",
    "            'last_char': 5,\n",
    "            'N': 5,\n",
    "            'use_padding': False,\n",
    "            'use_word_as_feature': False,\n",
    "            'tag_filter': (\"NOUN\", \"VERB\"),\n",
    "            'data_path': data_path,\n",
    "            'encoding': 'latin2',\n",
    "            'min_sample_per_class': 3,\n",
    "            'max_sample_per_class': 3000,\n",
    "            'max_lines': 200000,\n",
    "        },\n",
    "        'ffnn': {\n",
    "            'layers': (300, ),\n",
    "            'optimizer': 'MomentumOptimizer',\n",
    "            'optimizer_kwargs': {\n",
    "                'learning_rate': 1,\n",
    "                'momentum': .1,\n",
    "            },\n",
    "            'gpu_memory_fraction': 1,\n",
    "            'epochs': 5000,\n",
    "        }\n",
    "})\n",
    "e.run_and_save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500 epochs, cvalue: 0.1694050133228302\n",
      "1000 epochs, cvalue: 0.11332681775093079\n",
      "1500 epochs, cvalue: 0.09956234693527222\n",
      "2000 epochs, cvalue: 0.1011488139629364\n",
      "2500 epochs, cvalue: 0.08270842581987381\n",
      "3000 epochs, cvalue: 0.07109708338975906\n",
      "3500 epochs, cvalue: 0.06922101229429245\n",
      "4000 epochs, cvalue: 0.06034207344055176\n",
      "4500 epochs, cvalue: 0.06162016838788986\n",
      "5000 epochs, cvalue: 0.05050254613161087\n",
      "Training accuracy: 0.9430909752845764\n",
      "Test accuracy: 0.9342857003211975\n",
      "CPU times: user 1min 4s, sys: 2.61 s, total: 1min 6s\n",
      "Wall time: 1min 28s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "e = Experiment({\n",
    "        'global': {\n",
    "            'verbose': True,\n",
    "        },\n",
    "        'featurizer': {\n",
    "            'last_char': 5,\n",
    "            'N': 2,\n",
    "            'use_word_as_feature': False,\n",
    "            'tag_filter': (\"NOUN\", \"VERB\"),\n",
    "            'data_path': data_path,\n",
    "            'encoding': 'latin2',\n",
    "            'min_sample_per_class': 3,\n",
    "            'max_sample_per_class': 30000,\n",
    "            'max_lines': 200000,\n",
    "        },\n",
    "        'ffnn': {\n",
    "            'layers': (40, 40, 40),\n",
    "            'batch_size': 1000,\n",
    "            'optimizer': 'MomentumOptimizer',\n",
    "            'optimizer_kwargs': {\n",
    "                'learning_rate': 1,\n",
    "                'momentum': .1,\n",
    "            },\n",
    "            'gpu_memory_fraction': 1,\n",
    "            'batch_size': 500,\n",
    "            'epochs': 5000,\n",
    "        }\n",
    "})\n",
    "\n",
    "e.run_and_save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-141-6021948a4c18>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'e = Experiment({\\n        \\'global\\': {\\n            \\'verbose\\': True,\\n        },\\n        \\'featurizer\\': {\\n            \\'last_char\\': 3,\\n            \\'N\\': 3,\\n            \\'use_padding\\': False,\\n            \\'use_word_as_feature\\': False,\\n            \\'tag_filter\\': (\"NOUN\", \"VERB\"),\\n            \\'data_path\\': data_path,\\n            \\'encoding\\': \\'latin2\\',\\n            \\'min_sample_per_class\\': 3,\\n            \\'max_sample_per_class\\': 30000,\\n            \\'max_lines\\': 200000,\\n        },\\n        \\'ffnn\\': {\\n            \\'layers\\': (30, 30),\\n            \\'optimizer\\': \\'MomentumOptimizer\\',\\n            \\'optimizer_kwargs\\': {\\n                \\'learning_rate\\': 1,\\n                \\'momentum\\': .1,\\n            },\\n            \\'gpu_memory_fraction\\': 1,\\n            \\'epochs\\': 5000,\\n        }\\n})\\ne.run_and_save()'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/mnt/store/judit/.virtualenvs/deep/lib/python3.4/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2113\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2114\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2115\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2116\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-59>\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n",
      "\u001b[0;32m/mnt/store/judit/.virtualenvs/deep/lib/python3.4/site-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/store/judit/.virtualenvs/deep/lib/python3.4/site-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1178\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m             \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m             \u001b[0mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-133-4348af8d9fe6>\u001b[0m in \u001b[0;36mrun_and_save\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun_and_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m         \u001b[0mtrain_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunning_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-133-4348af8d9fe6>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_mask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m         \u001b[0mcvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mffnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdotrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m         \u001b[0mtrain_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mffnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdotest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0mtest_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mffnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdotest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-133-4348af8d9fe6>\u001b[0m in \u001b[0;36mdotrain\u001b[0;34m(self, X_train, y_train)\u001b[0m\n\u001b[1;32m     65\u001b[0m                 \u001b[0my_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             cvalues.append(self.sess.run([self.train, self.cost] + self.W + self.bias,\n\u001b[0;32m---> 67\u001b[0;31m                     feed_dict={self.n_input: X_batch, self.n_output: y_batch}))\n\u001b[0m\u001b[1;32m     68\u001b[0m             \u001b[0mcnt\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcnt\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/store/judit/.virtualenvs/deep/lib/python3.4/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    715\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    716\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 717\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    718\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    719\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/store/judit/.virtualenvs/deep/lib/python3.4/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    886\u001b[0m                 ' to a larger type (e.g. int64).')\n\u001b[1;32m    887\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 888\u001b[0;31m           \u001b[0mnp_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubfeed_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubfeed_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    889\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    890\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msubfeed_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp_val\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/store/judit/.virtualenvs/deep/lib/python3.4/site-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \"\"\"\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0masanyarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "e = Experiment({\n",
    "        'global': {\n",
    "            'verbose': True,\n",
    "        },\n",
    "        'featurizer': {\n",
    "            'last_char': 3,\n",
    "            'N': 3,\n",
    "            'use_padding': False,\n",
    "            'use_word_as_feature': False,\n",
    "            'tag_filter': (\"NOUN\", \"VERB\"),\n",
    "            'data_path': data_path,\n",
    "            'encoding': 'latin2',\n",
    "            'min_sample_per_class': 3,\n",
    "            'max_sample_per_class': 30000,\n",
    "            'max_lines': 200000,\n",
    "        },\n",
    "        'ffnn': {\n",
    "            'layers': (30, 30),\n",
    "            'batch_size': 1000,\n",
    "            'optimizer': 'MomentumOptimizer',\n",
    "            'optimizer_kwargs': {\n",
    "                'learning_rate': 1,\n",
    "                'momentum': .1,\n",
    "            },\n",
    "            'gpu_memory_fraction': 1,\n",
    "            'epochs': 5000,\n",
    "        }\n",
    "})\n",
    "e.run_and_save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## use last 10 characters of each word as a single feature\n",
    "\n",
    "Basically, we're building a memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500 epochs, cvalue: 0.2523937225341797\n",
      "1000 epochs, cvalue: 0.24883531033992767\n",
      "1500 epochs, cvalue: 0.247022345662117\n",
      "2000 epochs, cvalue: 0.24572668969631195\n",
      "2500 epochs, cvalue: 0.2446475476026535\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-142-342680fd8ee8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m })\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_and_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-133-4348af8d9fe6>\u001b[0m in \u001b[0;36mrun_and_save\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun_and_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m         \u001b[0mtrain_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunning_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-133-4348af8d9fe6>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_mask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m         \u001b[0mcvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mffnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdotrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m         \u001b[0mtrain_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mffnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdotest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0mtest_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mffnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdotest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-133-4348af8d9fe6>\u001b[0m in \u001b[0;36mdotrain\u001b[0;34m(self, X_train, y_train)\u001b[0m\n\u001b[1;32m     65\u001b[0m                 \u001b[0my_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             cvalues.append(self.sess.run([self.train, self.cost] + self.W + self.bias,\n\u001b[0;32m---> 67\u001b[0;31m                     feed_dict={self.n_input: X_batch, self.n_output: y_batch}))\n\u001b[0m\u001b[1;32m     68\u001b[0m             \u001b[0mcnt\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcnt\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/store/judit/.virtualenvs/deep/lib/python3.4/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    715\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    716\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 717\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    718\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    719\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/store/judit/.virtualenvs/deep/lib/python3.4/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    886\u001b[0m                 ' to a larger type (e.g. int64).')\n\u001b[1;32m    887\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 888\u001b[0;31m           \u001b[0mnp_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubfeed_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubfeed_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    889\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    890\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msubfeed_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp_val\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/store/judit/.virtualenvs/deep/lib/python3.4/site-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \"\"\"\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0masanyarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "e = Experiment({\n",
    "        'global': {\n",
    "            'verbose': True,\n",
    "        },\n",
    "        'featurizer': {\n",
    "            'last_char': 10,\n",
    "            'N': 1,\n",
    "            'use_word_as_feature': True,\n",
    "            \n",
    "            'tag_filter': (\"NOUN\", \"VERB\"),\n",
    "            'data_path': data_path,\n",
    "            'encoding': 'latin2',\n",
    "            'min_sample_per_class': 3,\n",
    "            'max_sample_per_class': 3000,\n",
    "            'max_lines': 200000,\n",
    "        },\n",
    "        'ffnn': {\n",
    "            'layers': (40, 40, 40),\n",
    "            'optimizer': 'MomentumOptimizer',\n",
    "            'optimizer_kwargs': {\n",
    "                'learning_rate': .1,\n",
    "                'momentum': .1,\n",
    "            },\n",
    "            'gpu_memory_fraction': .2,\n",
    "            'epochs': 5000,\n",
    "        }\n",
    "})\n",
    "\n",
    "e.run_and_save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-143-fbf3f242e039>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m             },\n\u001b[1;32m     24\u001b[0m             \u001b[0;34m'gpu_memory_fraction'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m.2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m             \u001b[0;34m'epochs'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m5000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         }\n\u001b[1;32m     27\u001b[0m })\n",
      "\u001b[0;32m<ipython-input-133-4348af8d9fe6>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    100\u001b[0m             \u001b[0mglob_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'global'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeaturizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFeaturizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'featurizer'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeaturizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeaturize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'featurizer'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-123-ce621fe823e1>\u001b[0m in \u001b[0;36mfeaturize\u001b[0;34m(self, data_path, encoding, min_sample_per_class, max_sample_per_class, max_lines, **kwargs)\u001b[0m\n\u001b[1;32m     40\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m                     \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract_word_and_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mFeaturizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidTag\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m                     \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-123-ce621fe823e1>\u001b[0m in \u001b[0;36mextract_word_and_tag\u001b[0;34m(self, line)\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0mfd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0mword\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfd\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m         \u001b[0mtag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfd\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull_tag\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "e = Experiment({\n",
    "        'global': {\n",
    "            'verbose': True,\n",
    "        },\n",
    "        'featurizer': {\n",
    "            'last_char': 10,\n",
    "            'N': 1,\n",
    "            'use_word_as_feature': True,\n",
    "            \n",
    "            'tag_filter': (\"NOUN\", \"VERB\"),\n",
    "            'data_path': data_path,\n",
    "            'encoding': 'latin2',\n",
    "            'min_sample_per_class': 3,\n",
    "            'max_sample_per_class': 300,\n",
    "            'max_lines': 200000,\n",
    "        },\n",
    "        'ffnn': {\n",
    "            'layers': (40, 40, 40),\n",
    "            'optimizer': 'MomentumOptimizer',\n",
    "            'optimizer_kwargs': {\n",
    "                'learning_rate': .1,\n",
    "                'momentum': .1,\n",
    "            },\n",
    "            'gpu_memory_fraction': .2,\n",
    "            'epochs': 5000,\n",
    "        }\n",
    "})\n",
    "\n",
    "e.run_and_save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
