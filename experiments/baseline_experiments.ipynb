{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ULM baseline experiments\n",
    "\n",
    "My first attempts at using tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sys import stdin\n",
    "from collections import defaultdict\n",
    "from scipy.io import mmread\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import re\n",
    "import datetime as dt\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from os import path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_path = '/mnt/store/hlt/Language/Hungarian/Crawl/Web2/ana/xaa.tagged'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class ResearchDiary:\n",
    "    diary_file = \"baseline_experiment_results.tsv\"\n",
    "        \n",
    "    @staticmethod\n",
    "    def load_or_create_dataframe():\n",
    "        if path.exists(ResearchDiary.diary_file):\n",
    "            result_diary = pd.read_table(ResearchDiary.diary_file)\n",
    "        else:\n",
    "            result_diary = pd.DataFrame(columns=[\n",
    "                                            'train_accuracy',\n",
    "                                            'test_accuracy',\n",
    "                                            'timestamp',\n",
    "                                            'data_path',\n",
    "                                            'theoretical_max',\n",
    "                                            'architecture',\n",
    "                                            'min_sample_per_class',\n",
    "                                            'max_sample_per_class',\n",
    "                                            'sample_count',\n",
    "                                            'feature_count',\n",
    "                                            'max_lines',\n",
    "                                            'N',\n",
    "                                            'last_char',\n",
    "                                            'full_tag',\n",
    "                                            'tag_filter',\n",
    "                                            'include_smaller_ngrams',\n",
    "                                            'use_padding',\n",
    "                                            'epochs',\n",
    "                                            'layers',\n",
    "                                            'activation',\n",
    "                                            'batch_size',\n",
    "                                            'optimizer',\n",
    "                                            'optimizer_kwargs',\n",
    "                                            'gpu_memory_fracion',\n",
    "                                            'running_time'\n",
    "                                        ])\n",
    "        return result_diary\n",
    "        \n",
    "    def __init__(self):\n",
    "        self.df = ResearchDiary.load_or_create_dataframe()\n",
    "        \n",
    "    def __enter__(self):\n",
    "        return self\n",
    "    \n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        self.df.to_csv(ResearchDiary.diary_file, sep='\\t', index=False)\n",
    "    \n",
    "    def add_experiment(self, data, create_cols_if_new=True):\n",
    "        new_index = len(self.df)\n",
    "        if create_cols_if_new:\n",
    "            for new_col in set(data.keys()) - set(self.df.columns):\n",
    "                self.df[new_col] = None\n",
    "        else:\n",
    "            missing_cols = set(data.keys()) - set(self.df.columns)\n",
    "            for c in missing_cols:\n",
    "                del data[c]\n",
    "        self.df = self.df.append(data, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import defaultdict\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "\n",
    "class Featurizer:\n",
    "    \n",
    "    class InvalidTag(Exception):\n",
    "        pass\n",
    "    \n",
    "    tag_re = re.compile(r'^[A-Z]+')\n",
    "    \n",
    "    def __init__(self, last_char, N, full_tag=False, use_padding=True, use_word_as_feature=False, tag_filter=None,\n",
    "                 include_smaller_ngrams=False, **kwargs):\n",
    "        self.last_char = last_char\n",
    "        self.N = N\n",
    "        self.full_tag = full_tag\n",
    "        self.use_padding = use_padding\n",
    "        self.use_word_as_feature = use_word_as_feature\n",
    "        self.include_smaller_ngrams = include_smaller_ngrams\n",
    "        if tag_filter is not None:\n",
    "            self.tag_filter = set(tag_filter)\n",
    "        else:\n",
    "            self.tag_filter = None\n",
    "        self.raw_input = []\n",
    "        \n",
    "    def featurize(self, data_path, encoding='utf-8', min_sample_per_class=0, max_sample_per_class=0, max_lines=0,\n",
    "                 **kwargs):\n",
    "        X = []\n",
    "        y = []\n",
    "        self.min_sample_per_class = min_sample_per_class\n",
    "        self.max_sample_per_class = max_sample_per_class\n",
    "        self.max_lines = max_lines\n",
    "        sample_cnt = defaultdict(int)\n",
    "        line_cnt = 0\n",
    "        with open(data_path, encoding=encoding) as f:\n",
    "            for line in f:\n",
    "                line_cnt += 1\n",
    "                if max_lines > 0 and line_cnt > max_lines:\n",
    "                    break\n",
    "                try:\n",
    "                    word, tag = self.extract_word_and_tag(line)\n",
    "                except Featurizer.InvalidTag:\n",
    "                    continue\n",
    "                if not word.strip() or not tag.strip():\n",
    "                    continue\n",
    "                if self.tag_filter is not None and tag not in self.tag_filter:\n",
    "                    continue\n",
    "                if max_sample_per_class > 0 and sample_cnt[tag] >= max_sample_per_class:\n",
    "                    continue\n",
    "                if all(v >= max_sample_per_class for v in sample_cnt.values()) and len(sample_cnt) > 1:\n",
    "                    break\n",
    "                self.__featurize_and_store_sample(word, tag, X, y)\n",
    "                self.raw_input.append((word, tag))\n",
    "                sample_cnt[tag] += 1\n",
    "        return self.create_feature_matrix(X, y)\n",
    "                \n",
    "    def create_feature_matrix(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.X_mtx = self.__get_or_create_vectorizer('X_vectorizer', X)\n",
    "        self.y_vec = self.__get_or_create_vectorizer('y_vectorizer', y)\n",
    "        return self.X_mtx, self.y_vec\n",
    "        \n",
    "    def __get_or_create_vectorizer(self, name, data):\n",
    "        if not hasattr(self, name):\n",
    "            dv = DictVectorizer()\n",
    "            v = dv.fit_transform(data)\n",
    "            setattr(self, name, dv)\n",
    "        return v\n",
    "    \n",
    "    def extract_word_and_tag(self, line):\n",
    "        fd = line.strip().split('\\t')\n",
    "        word = fd[0]\n",
    "        tag = fd[-1].split('/')[-1]\n",
    "        if self.full_tag is False:\n",
    "            try:\n",
    "                tag = Featurizer.tag_re.match(tag).group(0)\n",
    "            except AttributeError:\n",
    "                raise Featurizer.InvalidTag()\n",
    "        return word, tag\n",
    "    \n",
    "    def __featurize_and_store_sample(self, word, tag, X, y):\n",
    "        if self.use_word_as_feature:\n",
    "            f = {'word': word}\n",
    "        else:\n",
    "            f = self.__featurize_ngram(word)\n",
    "        X.append(f)\n",
    "        y.append({'class': tag})\n",
    "            \n",
    "    def __featurize_ngram(self, word):\n",
    "        feats = {}\n",
    "        if self.last_char > 0:\n",
    "            word = word[-self.last_char:]\n",
    "        if self.include_smaller_ngrams:\n",
    "            for n in range(1, self.N+1):\n",
    "                feats.update(Featurizer.extract_ngrams(word, n, self.use_padding))\n",
    "        else:\n",
    "            feats.update(Featurizer.extract_ngrams(word, self.N, self.use_padding))\n",
    "        return feats\n",
    "    \n",
    "    @staticmethod\n",
    "    def extract_ngrams(text, N, padding=False):\n",
    "        if padding is True:\n",
    "            text = '{0}{1}{0}'.format(\" \" * (N-1), text)\n",
    "        feats = {}\n",
    "        for i in range(len(text)-N+1):\n",
    "            feats['{0}_{1}'.format(N, i)] = text[i:i+N]\n",
    "        return feats\n",
    "            \n",
    "    def get_theoretical_max(self):\n",
    "        samples = defaultdict(lambda: defaultdict(int))\n",
    "        for i in range(len(self.X)):\n",
    "            xi = self.X[i]\n",
    "            yi = self.y[i]\n",
    "            f_str = ','.join('{}:{}'.format(feat, val) for feat, val in sorted(xi.items()))\n",
    "            samples[f_str][yi['class']] += 1\n",
    "        return sum(max(v.values()) for v in samples.values()) / len(self.X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "from scipy.sparse import issparse\n",
    "\n",
    "\n",
    "class FFNN:\n",
    "    \n",
    "    def __init__(self, n_feature, n_class, layers, batch_size=0, epochs=5000, verbose=False,\n",
    "                 activation=tf.sigmoid, gpu_memory_fraction=0.5,\n",
    "                 optimizer=\"GradientDescentOptimizer\", optimizer_kwargs={}):\n",
    "        self.n_feature = n_feature\n",
    "        self.n_class = n_class\n",
    "        self.shape = layers\n",
    "        self.batch_size = batch_size\n",
    "        self.n_input = tf.placeholder(tf.float32, shape=[None, n_feature],\n",
    "                         name=\"n_input\")\n",
    "        self.n_output = tf.placeholder(tf.float32, shape=[None, n_class],\n",
    "                          name=\"n_output\")\n",
    "        self.bias = []\n",
    "        self.W = []\n",
    "        self.hidden = []\n",
    "        self.activation = activation\n",
    "        self.create_input_layer()\n",
    "        self.create_hidden_layers()\n",
    "        self.create_output_layer()\n",
    "        self.cost = tf.reduce_mean(tf.square(self.n_output - self.output))\n",
    "        self.optimizer_kwargs = optimizer_kwargs\n",
    "        self.optimizer = getattr(tf.train, optimizer)(**optimizer_kwargs)\n",
    "        self.train = self.optimizer.minimize(self.cost)\n",
    "        self.init = tf.initialize_all_variables()\n",
    "        self.gpu_memory_fraction = gpu_memory_fraction\n",
    "        gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=gpu_memory_fraction)\n",
    "        self.sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))\n",
    "        self.sess.run(self.init)\n",
    "        self.verbose = verbose\n",
    "        self.epochs = epochs\n",
    "        \n",
    "    def create_input_layer(self):\n",
    "        self.bias.append(tf.Variable(tf.random_normal([self.shape[0]]), name=\"bias0\"))\n",
    "        self.W.append(tf.Variable(tf.random_normal([self.n_feature, self.shape[0]]), name=\"weights0\"))\n",
    "        self.hidden.append(self.activation(tf.matmul(self.n_input, self.W[0]) + self.bias[0]))\n",
    "        \n",
    "    def create_hidden_layers(self):\n",
    "        for i in range(1, len(self.shape)):\n",
    "            self.bias.append(tf.Variable(tf.random_normal([self.shape[i]]), name=\"bias{}\".format(i)))\n",
    "            self.W.append(tf.Variable(tf.random_normal([self.shape[i-1], self.shape[i]]), name=\"weights0\"))\n",
    "            self.hidden.append(self.activation(tf.matmul(self.hidden[-1], self.W[-1]) + self.bias[-1]))\n",
    "            \n",
    "    def create_output_layer(self):\n",
    "        self.bias.append(tf.Variable(tf.random_normal([self.n_class]), name=\"bias{}\".format(len(self.shape)-1)))\n",
    "        self.W.append(tf.Variable(tf.random_normal([self.shape[-1], self.n_class]), name=\"weights{}\".format(len(self.shape)-1)))\n",
    "        self.output = self.activation(tf.matmul(self.hidden[-1], self.W[-1]) + self.bias[-1])\n",
    "        \n",
    "    def dotrain(self, X_train, y_train):\n",
    "        X_train = FFNN.convert_sparse_if_needed(X_train)\n",
    "        y_train = FFNN.convert_sparse_if_needed(y_train)\n",
    "        cvalues = []\n",
    "        step = self.epochs // 10\n",
    "        step = 1 if step < 1 else step\n",
    "        cnt = 0\n",
    "        for epoch in range(0, self.epochs):\n",
    "            if self.batch_size > 0:\n",
    "                X_batch, y_batch = self.get_minibatch(X_train, y_train)\n",
    "            else:\n",
    "                X_batch = X_train\n",
    "                y_batch = y_train\n",
    "            cvalues.append(self.sess.run([self.train, self.cost] + self.W + self.bias,\n",
    "                    feed_dict={self.n_input: X_batch, self.n_output: y_batch}))\n",
    "            cnt += 1\n",
    "            if cnt % step == 0 and self.verbose is True:\n",
    "                print('{0} epochs, cvalue: {1}'.format(epoch+1, cvalues[-1][1]))\n",
    "        self.correct_prediction = tf.equal(tf.argmax(self.n_output,1), tf.argmax(self.output,1))\n",
    "        return cvalues\n",
    "        \n",
    "    def get_minibatch(self, X, y):\n",
    "        batch_index = np.random.choice(np.arange(0, X.shape[0]), self.batch_size)\n",
    "        return X[batch_index], y[batch_index]\n",
    "    \n",
    "    def dotest(self, X_test, y_test):\n",
    "        X_test = FFNN.convert_sparse_if_needed(X_test)\n",
    "        y_test = FFNN.convert_sparse_if_needed(y_test)\n",
    "        accuracy = tf.reduce_mean(tf.cast(self.correct_prediction, tf.float32))\n",
    "        prediction = self.correct_prediction.eval(session=self.sess,\n",
    "                                                  feed_dict={self.n_input: X_test, self.n_output: y_test})\n",
    "        return accuracy.eval(session=self.sess, feed_dict={self.n_input: X_test, self.n_output: y_test}), prediction\n",
    "        \n",
    "    @staticmethod\n",
    "    def convert_sparse_if_needed(mtx):\n",
    "        if issparse(mtx):\n",
    "            mtx = mtx.todense()\n",
    "        return mtx\n",
    "    \n",
    "\n",
    "class Experiment:\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        if 'global' not in config:\n",
    "            glob_config = {}\n",
    "        else:\n",
    "            glob_config = config['global']\n",
    "        self.featurizer = Featurizer(**config['featurizer'])\n",
    "        X, y = self.featurizer.featurize(**config['featurizer'])\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.test_size = glob_config.get('test_size', .1)\n",
    "        verbose = glob_config.get('verbose', False)\n",
    "        self.ffnn = FFNN(X.shape[1], y.shape[1], verbose=verbose, **config['ffnn'])\n",
    "        \n",
    "    def run(self):\n",
    "        self.train_mask = np.random.random(size=self.X.shape[0]) > self.test_size\n",
    "        X_train = self.X[self.train_mask]\n",
    "        X_test = self.X[np.invert(self.train_mask)]\n",
    "        y_train = self.y[self.train_mask]\n",
    "        y_test = self.y[np.invert(self.train_mask)]\n",
    "        cvalues = self.ffnn.dotrain(X_train, y_train)\n",
    "        train_acc, train_pred = self.ffnn.dotest(X_train, y_train)\n",
    "        test_acc, test_pred = self.ffnn.dotest(X_test, y_test)\n",
    "        self.test_pred = test_pred\n",
    "        self.train_acc = train_acc\n",
    "        self.test_acc = test_acc\n",
    "        return train_acc, train_pred, test_acc, test_pred\n",
    "    \n",
    "    def run_decision_tree(self):\n",
    "        self.clf = DecisionTreeClassifier()\n",
    "        return cross_val_score(self.clf, self.X.toarray(), self.y.toarray(), cv=10)\n",
    "        \n",
    "    def get_test_errors(self):\n",
    "        test_samples = [p[1] for p in filter(lambda x: not self.train_mask[x[0]], enumerate(self.featurizer.raw_input))]\n",
    "        errors = []\n",
    "        for i, s in enumerate(test_samples):\n",
    "            if not self.test_pred[i]:\n",
    "                errors.append(s)\n",
    "        return errors \n",
    "    \n",
    "    def add_results_to_diary(self, diary):\n",
    "        d = {\n",
    "            'timestamp': dt.datetime.now(),\n",
    "            'data_path': data_path,\n",
    "            'last_char': self.featurizer.last_char,\n",
    "            'N': self.featurizer.N,\n",
    "            'full_tag': self.featurizer.full_tag,\n",
    "            'use_padding': self.featurizer.use_padding,\n",
    "            'include_smaller_ngrams': self.featurizer.include_smaller_ngrams,\n",
    "            'tag_filter': self.featurizer.tag_filter,\n",
    "            'min_sample_per_class': self.featurizer.min_sample_per_class,\n",
    "            'max_sample_per_class': self.featurizer.max_sample_per_class,\n",
    "            'max_lines': self.featurizer.max_lines,\n",
    "            'sample_count': self.featurizer.X_mtx.shape[0],\n",
    "            'feature_count': self.featurizer.X_mtx.shape[1],\n",
    "            'architecture': 'FFNN',\n",
    "            'layers': self.ffnn.shape,\n",
    "            'batch_size': self.ffnn.batch_size,\n",
    "            'epochs': self.ffnn.epochs,\n",
    "            'activation': self.ffnn.activation.__name__,\n",
    "            'gpu_memory_fracion': self.ffnn.gpu_memory_fraction,\n",
    "            'optimizer': self.ffnn.optimizer.__class__,\n",
    "            'optimizer_kwargs': self.ffnn.optimizer_kwargs,\n",
    "            'train_accuracy': self.train_acc,\n",
    "            'test_accuracy': self.test_acc,\n",
    "            'theoretical_max': self.featurizer.get_theoretical_max(),\n",
    "            'running_time': self.running_time,\n",
    "        }\n",
    "        if 'global' in self.config and 'comment' in self.config['global']:\n",
    "            d['comment'] = self.config['global']['comment']\n",
    "        diary.add_experiment(d)\n",
    "    \n",
    "    def run_and_save(self):\n",
    "        start = dt.datetime.now()\n",
    "        train_acc, train_pred, test_acc, test_pred = self.run()\n",
    "        end = dt.datetime.now()\n",
    "        self.running_time = end - start\n",
    "        print(\"Training accuracy: {0}\\nTest accuracy: {1}\".format(train_acc, test_acc))\n",
    "        with ResearchDiary() as rd:\n",
    "            self.add_results_to_diary(rd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 epochs, cvalue: 0.4877081513404846\n",
      "10 epochs, cvalue: 0.4663439393043518\n",
      "15 epochs, cvalue: 0.3880554437637329\n",
      "20 epochs, cvalue: 0.37930601835250854\n",
      "25 epochs, cvalue: 0.3763933479785919\n",
      "30 epochs, cvalue: 0.36746475100517273\n",
      "35 epochs, cvalue: 0.3538150489330292\n",
      "40 epochs, cvalue: 0.3549867868423462\n",
      "45 epochs, cvalue: 0.3539010286331177\n",
      "50 epochs, cvalue: 0.35251584649086\n",
      "Training accuracy: 0.5105263590812683\n",
      "Test accuracy: 0.30000001192092896\n",
      "CPU times: user 3.24 s, sys: 524 ms, total: 3.76 s\n",
      "Wall time: 3.67 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "e = Experiment({\n",
    "        'global': {\n",
    "            'verbose': True,\n",
    "            'comment': \"toy example used for testing the framework\"\n",
    "        },\n",
    "        'featurizer': {\n",
    "            'last_char': 8,\n",
    "            'N': 1,\n",
    "            'use_padding': True,\n",
    "            'include_smaller_ngrams': True,\n",
    "            'tag_filter': (\"NOUN\", \"VERB\"),\n",
    "            'data_path': data_path,\n",
    "            'encoding': 'latin2',\n",
    "            'min_sample_per_class': 100,\n",
    "            'max_sample_per_class': 100,\n",
    "            'max_lines': 2000000,\n",
    "        },\n",
    "        'ffnn': {\n",
    "            'layers': (40, 40, 40),\n",
    "            'optimizer': 'MomentumOptimizer',\n",
    "            'batch_size': 1000,\n",
    "            'optimizer_kwargs': {\n",
    "                'learning_rate': 1,\n",
    "                'momentum': .1,\n",
    "            },\n",
    "            'gpu_memory_fraction': 1,\n",
    "            'epochs': 50,\n",
    "        }\n",
    "})\n",
    "e.run_and_save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.85,  0.45,  0.7 ,  0.75,  0.75,  0.75,  0.6 ,  0.85,  0.4 ,  0.9 ])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e.run_decision_tree()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>test_accuracy</th>\n",
       "      <th>sample_count</th>\n",
       "      <th>feature_count</th>\n",
       "      <th>last_char</th>\n",
       "      <th>N</th>\n",
       "      <th>use_padding</th>\n",
       "      <th>tag_filter</th>\n",
       "      <th>include_smaller_ngrams</th>\n",
       "      <th>layers</th>\n",
       "      <th>epochs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.502825</td>\n",
       "      <td>0.478261</td>\n",
       "      <td>200.0</td>\n",
       "      <td>207.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>{'VERB', 'NOUN'}</td>\n",
       "      <td>True</td>\n",
       "      <td>(40, 40, 40)</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.510526</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>200.0</td>\n",
       "      <td>207.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>{'NOUN', 'VERB'}</td>\n",
       "      <td>True</td>\n",
       "      <td>(40, 40, 40)</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.955026</td>\n",
       "      <td>0.952151</td>\n",
       "      <td>100000.0</td>\n",
       "      <td>437.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>{'NOUN', 'VERB'}</td>\n",
       "      <td>False</td>\n",
       "      <td>(40, 40)</td>\n",
       "      <td>10000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.979138</td>\n",
       "      <td>0.929889</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>1773.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>True</td>\n",
       "      <td>{'NOUN', 'VERB'}</td>\n",
       "      <td>False</td>\n",
       "      <td>(40, 40, 40)</td>\n",
       "      <td>5000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.740301</td>\n",
       "      <td>0.685155</td>\n",
       "      <td>6000.0</td>\n",
       "      <td>2414.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>False</td>\n",
       "      <td>{'NOUN', 'VERB'}</td>\n",
       "      <td>False</td>\n",
       "      <td>(300,)</td>\n",
       "      <td>5000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.966052</td>\n",
       "      <td>0.951476</td>\n",
       "      <td>48882.0</td>\n",
       "      <td>4059.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>True</td>\n",
       "      <td>{'NOUN', 'VERB'}</td>\n",
       "      <td>False</td>\n",
       "      <td>(40, 40, 40)</td>\n",
       "      <td>10000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.421262</td>\n",
       "      <td>0.423828</td>\n",
       "      <td>30454.0</td>\n",
       "      <td>579.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>(30, 30)</td>\n",
       "      <td>5000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.696985</td>\n",
       "      <td>0.671896</td>\n",
       "      <td>15000.0</td>\n",
       "      <td>4961.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>False</td>\n",
       "      <td>{'ADV', 'VERB', 'NOUN', 'ADJ', 'CONJ'}</td>\n",
       "      <td>True</td>\n",
       "      <td>(30, 30)</td>\n",
       "      <td>10000.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   train_accuracy  test_accuracy  sample_count  feature_count  last_char    N  \\\n",
       "0        0.502825       0.478261         200.0          207.0        8.0  1.0   \n",
       "1        0.510526       0.300000         200.0          207.0        8.0  1.0   \n",
       "2        0.955026       0.952151      100000.0          437.0        5.0  1.0   \n",
       "3        0.979138       0.929889        5000.0         1773.0        5.0  2.0   \n",
       "4        0.740301       0.685155        6000.0         2414.0        5.0  5.0   \n",
       "5        0.966052       0.951476       48882.0         4059.0        5.0  2.0   \n",
       "6        0.421262       0.423828       30454.0          579.0       10.0  1.0   \n",
       "7        0.696985       0.671896       15000.0         4961.0       10.0  2.0   \n",
       "\n",
       "  use_padding                              tag_filter include_smaller_ngrams  \\\n",
       "0        True                        {'VERB', 'NOUN'}                   True   \n",
       "1        True                        {'NOUN', 'VERB'}                   True   \n",
       "2        True                        {'NOUN', 'VERB'}                  False   \n",
       "3        True                        {'NOUN', 'VERB'}                  False   \n",
       "4       False                        {'NOUN', 'VERB'}                  False   \n",
       "5        True                        {'NOUN', 'VERB'}                  False   \n",
       "6       False                                     NaN                  False   \n",
       "7       False  {'ADV', 'VERB', 'NOUN', 'ADJ', 'CONJ'}                   True   \n",
       "\n",
       "         layers   epochs  \n",
       "0  (40, 40, 40)     50.0  \n",
       "1  (40, 40, 40)     50.0  \n",
       "2      (40, 40)  10000.0  \n",
       "3  (40, 40, 40)   5000.0  \n",
       "4        (300,)   5000.0  \n",
       "5  (40, 40, 40)  10000.0  \n",
       "6      (30, 30)   5000.0  \n",
       "7      (30, 30)  10000.0  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rd = ResearchDiary()\n",
    "rd.df[['train_accuracy', 'test_accuracy', 'sample_count', 'feature_count', 'last_char', 'N', 'use_padding', 'tag_filter', 'include_smaller_ngrams', 'layers', 'epochs']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['train_accuracy', 'test_accuracy', 'timestamp', 'data_path',\n",
       "       'theoretical_max', 'architecture', 'min_sample_per_class',\n",
       "       'max_sample_per_class', 'sample_count', 'feature_count', 'max_lines',\n",
       "       'N', 'last_char', 'full_tag', 'tag_filter', 'include_smaller_ngrams',\n",
       "       'use_padding', 'epochs', 'layers', 'activation', 'batch_size',\n",
       "       'optimizer', 'optimizer_kwargs', 'gpu_memory_fracion', 'running_time',\n",
       "       'comment'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rd.df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 epochs, cvalue: 0.11534780263900757\n",
      "2000 epochs, cvalue: 0.08339373767375946\n",
      "3000 epochs, cvalue: 0.06078900769352913\n",
      "4000 epochs, cvalue: 0.060510020703077316\n",
      "5000 epochs, cvalue: 0.061203502118587494\n",
      "6000 epochs, cvalue: 0.04734017699956894\n",
      "7000 epochs, cvalue: 0.04871213063597679\n",
      "8000 epochs, cvalue: 0.04383450001478195\n",
      "9000 epochs, cvalue: 0.040955979377031326\n",
      "10000 epochs, cvalue: 0.042818546295166016\n",
      "Training accuracy: 0.9550263285636902\n",
      "Test accuracy: 0.9521514773368835\n",
      "CPU times: user 46.7 s, sys: 2.94 s, total: 49.6 s\n",
      "Wall time: 36 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "e = Experiment({\n",
    "        'global': {\n",
    "            'verbose': True,\n",
    "        },\n",
    "        'featurizer': {\n",
    "            'last_char': 5,\n",
    "            'N': 1,\n",
    "            'use_word_as_feature': False,\n",
    "            'tag_filter': (\"NOUN\", \"VERB\"),\n",
    "            'data_path': data_path,\n",
    "            'encoding': 'latin2',\n",
    "            'min_sample_per_class': 3,\n",
    "            'max_sample_per_class': 50000,\n",
    "            'max_lines': 2000000,\n",
    "        },\n",
    "        'ffnn': {\n",
    "            'layers': (40, 40),\n",
    "            'batch_size': 1000,\n",
    "            'optimizer': 'MomentumOptimizer',\n",
    "            'optimizer_kwargs': {\n",
    "                'learning_rate': 1,\n",
    "                'momentum': .1,\n",
    "            },\n",
    "            'gpu_memory_fraction': 1,\n",
    "            'epochs': 10000,\n",
    "        }\n",
    "})\n",
    "e.run_and_save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('egésze', 'NOUN'),\n",
       " ('fölöttük', 'NOUN'),\n",
       " ('köze', 'NOUN'),\n",
       " ('mindezek', 'NOUN'),\n",
       " ('mellette', 'NOUN'),\n",
       " ('bizonyíték', 'NOUN'),\n",
       " ('vízum', 'NOUN'),\n",
       " ('egybecseng', 'VERB'),\n",
       " ('hömpölyög', 'VERB'),\n",
       " ('körbeér', 'VERB'),\n",
       " ('keveri', 'VERB'),\n",
       " ('állattá', 'NOUN'),\n",
       " ('besétál', 'VERB'),\n",
       " ('ennél', 'NOUN'),\n",
       " ('Hérakleitosznál', 'NOUN'),\n",
       " ('tűzzel', 'NOUN'),\n",
       " ('kár', 'NOUN'),\n",
       " ('annak', 'NOUN'),\n",
       " ('kivétellel', 'NOUN'),\n",
       " ('közéjük', 'NOUN')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "errors = e.get_test_errors()\n",
    "errors[:10]\n",
    "d = defaultdict(int)\n",
    "for i in errors:\n",
    "    d[i[1]] += 1\n",
    "d\n",
    "errors[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500 epochs, cvalue: 0.15165449678897858\n",
      "1000 epochs, cvalue: 0.08083382248878479\n",
      "1500 epochs, cvalue: 0.07529635727405548\n",
      "2000 epochs, cvalue: 0.0778464823961258\n",
      "2500 epochs, cvalue: 0.06110145524144173\n",
      "3000 epochs, cvalue: 0.03207963705062866\n",
      "3500 epochs, cvalue: 0.023041512817144394\n",
      "4000 epochs, cvalue: 0.022178513929247856\n",
      "4500 epochs, cvalue: 0.02886219508945942\n",
      "5000 epochs, cvalue: 0.01383767370134592\n",
      "Training accuracy: 0.9791384935379028\n",
      "Test accuracy: 0.9298893213272095\n",
      "CPU times: user 16.4 s, sys: 720 ms, total: 17.2 s\n",
      "Wall time: 12.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "e = Experiment({\n",
    "        'global': {\n",
    "            'verbose': True,\n",
    "        },\n",
    "        'featurizer': {\n",
    "            'last_char': 5,\n",
    "            'N': 2,\n",
    "            'use_word_as_feature': False,\n",
    "            'tag_filter': (\"NOUN\", \"VERB\"),\n",
    "            'data_path': data_path,\n",
    "            'encoding': 'latin2',\n",
    "            'min_sample_per_class': 3,\n",
    "            'max_sample_per_class': 2500,\n",
    "            'max_lines': 2000000,\n",
    "        },\n",
    "        'ffnn': {\n",
    "            'layers': (40, 40, 40),\n",
    "            'batch_size': 100,\n",
    "            'optimizer': 'MomentumOptimizer',\n",
    "            'optimizer_kwargs': {\n",
    "                'learning_rate': 1,\n",
    "                'momentum': .1,\n",
    "            },\n",
    "            'gpu_memory_fraction': 1,\n",
    "            'epochs': 5000,\n",
    "        }\n",
    "})\n",
    "e.run_and_save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500 epochs, cvalue: 0.39113083481788635\n",
      "1000 epochs, cvalue: 0.3751225173473358\n",
      "1500 epochs, cvalue: 0.3640808165073395\n",
      "2000 epochs, cvalue: 0.3556898832321167\n",
      "2500 epochs, cvalue: 0.34844157099723816\n",
      "3000 epochs, cvalue: 0.34273603558540344\n",
      "3500 epochs, cvalue: 0.27323392033576965\n",
      "4000 epochs, cvalue: 0.22539880871772766\n",
      "4500 epochs, cvalue: 0.20112769305706024\n",
      "5000 epochs, cvalue: 0.18312877416610718\n",
      "Training accuracy: 0.7403007745742798\n",
      "Test accuracy: 0.6851550340652466\n",
      "CPU times: user 2min 50s, sys: 50.3 s, total: 3min 41s\n",
      "Wall time: 3min 41s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "e = Experiment({\n",
    "        'global': {\n",
    "            'verbose': True,\n",
    "            'comment': \"memory\",\n",
    "        },\n",
    "        'featurizer': {\n",
    "            'last_char': 5,\n",
    "            'N': 5,\n",
    "            'use_padding': False,\n",
    "            'use_word_as_feature': False,\n",
    "            'tag_filter': (\"NOUN\", \"VERB\"),\n",
    "            'data_path': data_path,\n",
    "            'encoding': 'latin2',\n",
    "            'min_sample_per_class': 3,\n",
    "            'max_sample_per_class': 3000,\n",
    "            'max_lines': 200000,\n",
    "        },\n",
    "        'ffnn': {\n",
    "            'layers': (300, ),\n",
    "            'optimizer': 'MomentumOptimizer',\n",
    "            'optimizer_kwargs': {\n",
    "                'learning_rate': 1,\n",
    "                'momentum': .1,\n",
    "            },\n",
    "            'gpu_memory_fraction': 1,\n",
    "            'epochs': 5000,\n",
    "        }\n",
    "})\n",
    "e.run_and_save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 epochs, cvalue: 0.1329595148563385\n",
      "2000 epochs, cvalue: 0.0919666439294815\n",
      "3000 epochs, cvalue: 0.07032275199890137\n",
      "4000 epochs, cvalue: 0.06475789844989777\n",
      "5000 epochs, cvalue: 0.06308519840240479\n",
      "6000 epochs, cvalue: 0.044318508356809616\n",
      "7000 epochs, cvalue: 0.03979392722249031\n",
      "8000 epochs, cvalue: 0.0352330207824707\n",
      "9000 epochs, cvalue: 0.02733471244573593\n",
      "10000 epochs, cvalue: 0.03111211583018303\n",
      "Training accuracy: 0.9660520553588867\n",
      "Test accuracy: 0.9514761567115784\n",
      "CPU times: user 1min 53s, sys: 3.72 s, total: 1min 56s\n",
      "Wall time: 1min 41s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "e = Experiment({\n",
    "        'global': {\n",
    "            'verbose': True,\n",
    "        },\n",
    "        'featurizer': {\n",
    "            'last_char': 5,\n",
    "            'N': 2,\n",
    "            'use_word_as_feature': False,\n",
    "            'tag_filter': (\"NOUN\", \"VERB\"),\n",
    "            'data_path': data_path,\n",
    "            'encoding': 'latin2',\n",
    "            'min_sample_per_class': 3,\n",
    "            'max_sample_per_class': 30000,\n",
    "            'max_lines': 200000,\n",
    "        },\n",
    "        'ffnn': {\n",
    "            'layers': (40, 40, 40),\n",
    "            'batch_size': 1000,\n",
    "            'optimizer': 'MomentumOptimizer',\n",
    "            'optimizer_kwargs': {\n",
    "                'learning_rate': 1,\n",
    "                'momentum': .1,\n",
    "            },\n",
    "            'gpu_memory_fraction': 1,\n",
    "            'batch_size': 500,\n",
    "            'epochs': 10000,\n",
    "        }\n",
    "})\n",
    "\n",
    "e.run_and_save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500 epochs, cvalue: 0.06266084313392639\n",
      "1000 epochs, cvalue: 0.06152767315506935\n",
      "1500 epochs, cvalue: 0.059499263763427734\n",
      "2000 epochs, cvalue: 0.05672406405210495\n",
      "2500 epochs, cvalue: 0.05543501302599907\n",
      "3000 epochs, cvalue: 0.05370552837848663\n",
      "3500 epochs, cvalue: 0.051537346094846725\n",
      "4000 epochs, cvalue: 0.05039873346686363\n",
      "4500 epochs, cvalue: 0.04706898331642151\n",
      "5000 epochs, cvalue: 0.0474327877163887\n",
      "Training accuracy: 0.4212619960308075\n",
      "Test accuracy: 0.4238280951976776\n",
      "CPU times: user 26.9 s, sys: 1.49 s, total: 28.4 s\n",
      "Wall time: 20.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "e = Experiment({\n",
    "        'global': {\n",
    "            'verbose': True,\n",
    "            'comment': \"no tag filter\"\n",
    "        },\n",
    "        'featurizer': {\n",
    "            'last_char': 10,\n",
    "            'N': 1,\n",
    "            'use_padding': False,\n",
    "            'use_word_as_feature': False,\n",
    "            'data_path': data_path,\n",
    "            'encoding': 'latin2',\n",
    "            'min_sample_per_class': 3,\n",
    "            'max_sample_per_class': 3000,\n",
    "            'max_lines': 200000,\n",
    "        },\n",
    "        'ffnn': {\n",
    "            'layers': (30, 30),\n",
    "            'batch_size': 1000,\n",
    "            'optimizer': 'MomentumOptimizer',\n",
    "            'optimizer_kwargs': {\n",
    "                'learning_rate': 1,\n",
    "                'momentum': .1,\n",
    "            },\n",
    "            'gpu_memory_fraction': 1,\n",
    "            'epochs': 5000,\n",
    "        }\n",
    "})\n",
    "e.run_and_save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>, {'PUNCT': 50, 'NOUN': 292, 'POSTP': 109, 'DET': 5, 'ADJ': 263, 'CONJ': 308, 'PREP': 2, 'ONO': 1, 'UTT': 88, 'VERB': 172, 'ART': 11, 'PREV': 130, 'ADV': 221, 'UNKNOWN': 5, 'NUM': 113})\n",
      "(30454, 579) (30454, 15)\n",
      "defaultdict(<class 'int'>, {'PUNCT': 3000, 'NOUN': 3000, 'NUM': 3000, 'DET': 31, 'ADJ': 3000, 'CONJ': 3000, 'PREP': 17, 'ONO': 10, 'UTT': 695, 'VERB': 3000, 'ART': 3000, 'PREV': 2657, 'ADV': 3000, 'UNKNOWN': 44, 'POSTP': 3000})\n"
     ]
    }
   ],
   "source": [
    "errors = e.get_test_errors()\n",
    "errors[:10]\n",
    "d = defaultdict(int)\n",
    "for i in errors:\n",
    "    d[i[1]] += 1\n",
    "print(d)\n",
    "errors[:20]\n",
    "print(e.featurizer.X_mtx.shape, e.featurizer.y_vec.shape)\n",
    "y = e.featurizer.y\n",
    "classes = defaultdict(int)\n",
    "for c in y:\n",
    "    classes[list(c.values())[0]] += 1\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 epochs, cvalue: 0.13049420714378357\n",
      "2000 epochs, cvalue: 0.11941518634557724\n",
      "3000 epochs, cvalue: 0.11810123175382614\n",
      "4000 epochs, cvalue: 0.10926355421543121\n",
      "5000 epochs, cvalue: 0.1102619618177414\n",
      "6000 epochs, cvalue: 0.09652210772037506\n",
      "7000 epochs, cvalue: 0.09469000995159149\n",
      "8000 epochs, cvalue: 0.09023628383874893\n",
      "9000 epochs, cvalue: 0.08500447869300842\n",
      "10000 epochs, cvalue: 0.0812755599617958\n",
      "Training accuracy: 0.6969850659370422\n",
      "Test accuracy: 0.6718963384628296\n",
      "CPU times: user 3min 40s, sys: 4.48 s, total: 3min 45s\n",
      "Wall time: 3min 29s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "e = Experiment({\n",
    "        'global': {\n",
    "            'verbose': True,\n",
    "        },\n",
    "        'featurizer': {\n",
    "            'last_char': 10,\n",
    "            'N': 2,\n",
    "            'use_padding': False,\n",
    "            'include_smaller_ngrams': True,\n",
    "            'tag_filter': [\"VERB\", \"NOUN\", \"ADJ\", \"CONJ\", \"ADV\"],\n",
    "            'data_path': data_path,\n",
    "            'encoding': 'latin2',\n",
    "            'min_sample_per_class': 300,\n",
    "            'max_sample_per_class': 3000,\n",
    "            'max_lines': 200000,\n",
    "        },\n",
    "        'ffnn': {\n",
    "            'layers': (30, 30),\n",
    "            'batch_size': 1000,\n",
    "            'optimizer': 'MomentumOptimizer',\n",
    "            'optimizer_kwargs': {\n",
    "                'learning_rate': 1,\n",
    "                'momentum': .1,\n",
    "            },\n",
    "            'gpu_memory_fraction': 1,\n",
    "            'epochs': 10000,\n",
    "        }\n",
    "})\n",
    "e.run_and_save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.78666667,  0.76733333,  0.79266667,  0.75266667,  0.80533333,\n",
       "        0.81733333,  0.86266667,  0.832     ,  0.862     ,  0.948     ])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e.run_decision_tree()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
