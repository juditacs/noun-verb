{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.layers.recurrent import LSTM, GRU, SimpleRNN\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from scipy.sparse import issparse\n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "plt.rcParams['figure.figsize'] = (25, 13)\n",
    "plt.rcParams['font.family'] = 'sans-serif'\n",
    "plt.rcParams['font.size'] = 20\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Featurizer:\n",
    "\n",
    "    class InvalidTag(Exception):\n",
    "        pass\n",
    "\n",
    "    tag_re = re.compile(r'^[A-Z]+')\n",
    "\n",
    "    def __init__(self, last_char, N, data_path, full_tag=False,\n",
    "                 use_padding=True,\n",
    "                 use_word_as_feature=False, tag_filter=None,\n",
    "                 include_smaller_ngrams=False,\n",
    "                 encoding='utf-8',\n",
    "                 sample_per_class=100,\n",
    "                 max_lines=2000000,\n",
    "                 grep_filter=None,\n",
    "                 uniq_lines=False,\n",
    "                 **kwargs):\n",
    "        self.last_char = last_char\n",
    "        self.N = N\n",
    "        self.full_tag = full_tag\n",
    "        self.use_padding = use_padding\n",
    "        self.use_word_as_feature = use_word_as_feature\n",
    "        self.include_smaller_ngrams = include_smaller_ngrams\n",
    "        self.data_path = data_path\n",
    "        self.sample_per_class = sample_per_class\n",
    "        self.max_lines = max_lines\n",
    "        self.encoding = encoding\n",
    "        if uniq_lines:\n",
    "            self.uniq_lines = set()\n",
    "        else:\n",
    "            self.uniq_lines = None\n",
    "        if tag_filter is not None:\n",
    "            self.tag_filter = set(tag_filter)\n",
    "        else:\n",
    "            self.tag_filter = None\n",
    "        self.grep_filter = grep_filter\n",
    "\n",
    "    def featurize(self):\n",
    "        X = []\n",
    "        y = []\n",
    "        sample_cnt = defaultdict(int)\n",
    "        line_cnt = 0\n",
    "        if self.grep_filter is not None:\n",
    "            categ_cnt = len(self.grep_filter) + 1\n",
    "        elif self.tag_filter:\n",
    "            categ_cnt = len(self.tag_filter)\n",
    "        else:\n",
    "            categ_cnt = 1\n",
    "        with open(self.data_path, encoding=self.encoding) as f:\n",
    "            for line in f:\n",
    "                line_cnt += 1\n",
    "                if self.max_lines > 0 and line_cnt > self.max_lines:\n",
    "                    break\n",
    "                try:\n",
    "                    word, tag, pos = self.extract_word_and_tag(line)\n",
    "                except Featurizer.InvalidTag:\n",
    "                    continue\n",
    "                if not word.strip() or not tag.strip():\n",
    "                    continue\n",
    "                if self.tag_filter is not None and pos not in self.tag_filter:\n",
    "                    continue\n",
    "                if self.sample_per_class > 0 and \\\n",
    "                    (all(v >= self.sample_per_class\n",
    "                         for v in sample_cnt.values()) and\n",
    "                     len(sample_cnt) >= categ_cnt):\n",
    "                    break\n",
    "                if self.tag_filter is None:\n",
    "                    pos = tag\n",
    "                if self.uniq_lines is not None:\n",
    "                    if (word, tag) in self.uniq_lines:\n",
    "                        continue\n",
    "                    self.uniq_lines.add((word, tag))\n",
    "                sample_cnt[(pos, tag)] += 1\n",
    "                if self.sample_per_class > 0 and sample_cnt[(pos, tag)] > self.sample_per_class:\n",
    "                    continue\n",
    "                self.__featurize_and_store_sample(word, tag, X, y)\n",
    "        print(\"READ {} lines\".format(line_cnt))\n",
    "        return self.create_feature_matrix(X, y)\n",
    "\n",
    "    def create_feature_matrix(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.X_mtx = self.__get_or_create_vectorizer('X_vectorizer', X)\n",
    "        self.y_vec = self.__get_or_create_vectorizer('y_vectorizer', y)\n",
    "        return self.X_mtx, self.y_vec\n",
    "    \n",
    "    def create_train_test(self, ratio):\n",
    "        Xtr, Xte, ytr, yte = train_test_split(self.X_mtx, self.y_vec, train_size=ratio)\n",
    "        self.X_train = Xtr\n",
    "        self.X_test = Xte\n",
    "        self.y_train = ytr\n",
    "        self.y_test = yte\n",
    "        \n",
    "    def __get_or_create_vectorizer(self, name, data):\n",
    "        if not hasattr(self, name):\n",
    "            dv = DictVectorizer()\n",
    "            v = dv.fit_transform(data)\n",
    "            setattr(self, name, dv)\n",
    "        return v\n",
    "\n",
    "    def extract_word_and_tag(self, line):\n",
    "        fd = line.strip().split('\\t')\n",
    "        if len(fd) > 1 and not '/' in fd[1]:\n",
    "            return (fd[0], fd[1], fd[1])\n",
    "        word = fd[0]\n",
    "        tag = fd[-1].split('/')[-1]\n",
    "        try:\n",
    "            pos = Featurizer.tag_re.match(tag).group(0)\n",
    "        except AttributeError:\n",
    "            raise Featurizer.InvalidTag()\n",
    "        if self.grep_filter is not None:\n",
    "            for cl in self.grep_filter:\n",
    "                if cl in tag:\n",
    "                    return word, cl, pos\n",
    "            return word, \"NONE\", pos\n",
    "        if self.full_tag is False:\n",
    "            return word, pos, pos\n",
    "        return word, tag, tag\n",
    "\n",
    "    def __featurize_and_store_sample(self, word, tag, X, y):\n",
    "        if self.use_word_as_feature:\n",
    "            f = {'word': word}\n",
    "        else:\n",
    "            f = self.__featurize_ngram(word)\n",
    "        X.append(f)\n",
    "        y.append({'class': tag})\n",
    "\n",
    "    def __featurize_ngram(self, word):\n",
    "        feats = {}\n",
    "        if self.last_char > 0:\n",
    "            word = word[-self.last_char:]\n",
    "        if self.include_smaller_ngrams:\n",
    "            for n in range(1, self.N+1):\n",
    "                feats.update(Featurizer.extract_ngrams(\n",
    "                    word, n, self.use_padding))\n",
    "        else:\n",
    "            feats.update(Featurizer.extract_ngrams(\n",
    "                word, self.N, self.use_padding))\n",
    "        return feats\n",
    "\n",
    "    @staticmethod\n",
    "    def extract_ngrams(text, N, padding=False):\n",
    "        if padding is True:\n",
    "            text = '{0}{1}{0}'.format(\" \" * (N-1), text)\n",
    "        feats = {}\n",
    "        for i in range(len(text)-N+1):\n",
    "            feats['{0}_{1}'.format(N, i)] = text[i:i+N]\n",
    "        return feats\n",
    "\n",
    "    def get_theoretical_max(self):\n",
    "        samples = defaultdict(lambda: defaultdict(int))\n",
    "        for i in range(len(self.X)):\n",
    "            xi = self.X[i]\n",
    "            yi = self.y[i]\n",
    "            f_str = ','.join('{}:{}'.format(feat, val)\n",
    "                             for feat, val in sorted(xi.items()))\n",
    "            samples[f_str][yi['class']] += 1\n",
    "        return sum(max(v.values()) for v in samples.values()) / len(self.X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class FFNN:\n",
    "    @staticmethod\n",
    "    def create_list_if_str(param, length):\n",
    "        if isinstance(param, str):\n",
    "            return [param] * length\n",
    "        return param\n",
    "    \n",
    "    def __init__(self, layers, activations, input_dim, output_dim, *,\n",
    "                 loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'],\n",
    "                nb_epoch=50, batch_size=50, early_stopping=True, **kwargs):\n",
    "        self.layers = layers\n",
    "        self.activations = FFNN.create_list_if_str(activations, len(self.layers) + 1)\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.early_stopping = early_stopping\n",
    "        self.model_fit_args = {\n",
    "            'nb_epoch': nb_epoch,\n",
    "            'batch_size': batch_size,\n",
    "        }\n",
    "        self.model_compile_args = {\n",
    "            'optimizer': optimizer,\n",
    "            'metrics': metrics,\n",
    "            'loss': loss,\n",
    "        }\n",
    "        self.create_network()\n",
    "        \n",
    "    def create_network(self):\n",
    "        self.model = Sequential()\n",
    "        # input layer\n",
    "        self.model.add(Dense(self.layers[0], input_dim=self.input_dim, activation=self.activations[0]))\n",
    "        for i in range(1, len(self.layers)):\n",
    "            self.model.add(Dense(self.layers[i], activation=self.activations[i]))\n",
    "        #output layer\n",
    "        self.model.add(Dense(self.output_dim, activation=self.activations[-1]))\n",
    "        self.model.compile(**self.model_compile_args)\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        X = FFNN.densify(X)\n",
    "        y = FFNN.densify(y)\n",
    "        start = datetime.now()\n",
    "        if self.early_stopping:\n",
    "            early_stopping = EarlyStopping(monitor='val_loss', patience=2)\n",
    "            self.model.fit(X, y, verbose=0, validation_split=0.2, callbacks=[early_stopping], **self.model_fit_args)\n",
    "        else:\n",
    "            self.model.fit(X, y, verbose=0, **self.model_fit_args)\n",
    "        return datetime.now() - start\n",
    "        \n",
    "    def evaluate(self, X, y, **kwargs):\n",
    "        X = FFNN.densify(X)\n",
    "        y = FFNN.densify(y)\n",
    "        return self.model.evaluate(X, y, **kwargs)\n",
    "    \n",
    "    @staticmethod\n",
    "    def densify(mtx):\n",
    "        if issparse(mtx):\n",
    "            return mtx.todense()\n",
    "        return mtx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Config:\n",
    "    def __init__(self, conf_d):\n",
    "        self.glob_conf = Config.defaults.get('global', {})\n",
    "        self.feat_conf = Config.defaults.get('featurizer', {})\n",
    "        self.model_conf = Config.defaults.get('model', {})\n",
    "        \n",
    "        self.glob_conf.update(conf_d.get('global', {}))\n",
    "        self.feat_conf.update(conf_d.get('featurizer', {}))\n",
    "        self.model_conf.update(conf_d.get('model', {}))\n",
    "        \n",
    "    def to_dict(self):\n",
    "        d = {}\n",
    "        d.update(self.serialize_config(self.glob_conf, 'global'))\n",
    "        d.update(self.serialize_config(self.feat_conf, 'feat'))\n",
    "        d.update(self.serialize_config(self.model_conf, 'model'))\n",
    "        return d\n",
    "        \n",
    "    def serialize_config(self, section, pre):\n",
    "        d = {}\n",
    "        for k, v in section.items():\n",
    "            d['{0}.{1}'.format(pre, k)] = v\n",
    "        return d\n",
    "        \n",
    "    defaults = {\n",
    "        'global': {\n",
    "            'train_test_split': .9,\n",
    "            'nolog': False,\n",
    "        },\n",
    "        'model': {\n",
    "            'loss': 'binary_crossentropy',\n",
    "            'optimizer': 'rmsprop',\n",
    "            'metrics': ['accuracy'],\n",
    "            'nb_epoch': 50,\n",
    "            'batch_size': 64,\n",
    "            'early_stopping': True,\n",
    "        },\n",
    "        'featurizer': {\n",
    "            'data_path': '/mnt/store/hlt/Language/Hungarian/Crawl/Web2/ana/xaa.tagged',\n",
    "            'encoding': 'latin2',\n",
    "            'include_smaller_ngrams': False,\n",
    "            'use_padding': True,\n",
    "            'tag_filter': (\"NOUN\", \"VERB\"),\n",
    "            'uniq_lines': False,\n",
    "        },\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Result:\n",
    "    __slots__ = ('train_sample_count', 'test_sample_count', 'feature_count', 'class_no',\n",
    "                 'success', 'exception',\n",
    "                 'running_time', 'timestamp',\n",
    "                 'train_acc', 'test_acc', 'train_loss', 'test_loss')\n",
    "    \n",
    "    def to_dict(self):\n",
    "        d = {}\n",
    "        for k in Result.__slots__:\n",
    "            try:\n",
    "                d['result.{}'.format(k)] = getattr(self, k)\n",
    "            except AttributeError:\n",
    "                d['result.{}'.format(k)] = None\n",
    "        return d\n",
    "    \n",
    "    def __str__(self):\n",
    "        if self.success:\n",
    "            return \"SUCCESS: Train accuracy: {}\\nTest accuracy: {}\".format(self.train_acc, self.test_acc)\n",
    "        else:\n",
    "            return \"FAIL: {}\".format(self.exception)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Experiment:\n",
    "    \n",
    "    df_path = 'new_results.tsv'\n",
    "    \n",
    "    def __init__(self, conf_d):\n",
    "        self.config = Config(conf_d)\n",
    "        self.featurizer = Featurizer(**self.config.feat_conf)\n",
    "        self.featurizer.featurize()\n",
    "        input_dim = self.featurizer.X_mtx.shape[1]\n",
    "        output_dim = self.featurizer.y_vec.shape[1]\n",
    "        self.config.model_conf['input_dim'] = input_dim\n",
    "        self.config.model_conf['output_dim'] = output_dim\n",
    "        self.initialize_model()\n",
    "        self.result = Result()\n",
    "        self.result.feature_count = input_dim\n",
    "        \n",
    "    def initialize_model(self):\n",
    "        if self.config.model_conf['architecture'].lower() == 'ffnn':\n",
    "            self.model = FFNN(**self.config.model_conf)\n",
    "        elif self.config.model_conf['architecture'].lower() == 'recurrent':\n",
    "            self.model = RecurrentNN(**self.config.model_conf)\n",
    "        elif self.config.model_conf['architecture'].lower() == 'autoencoder':\n",
    "            self.model = AutoEncoder(**self.config.model_conf)\n",
    "        else:\n",
    "            raise ValueError(\"Model architecture [{}] not supported\".format(\n",
    "                    self.config.model_conf['architecture']))\n",
    "            \n",
    "    def fit_train(self):\n",
    "        self.result.timestamp = datetime.now()\n",
    "        self.featurizer.create_train_test(self.config.glob_conf['train_test_split'])\n",
    "        rt = self.model.fit(self.featurizer.X_train.todense(), self.featurizer.y_train.todense())\n",
    "        self.result.running_time = rt\n",
    "        \n",
    "    def evaluate_train(self):\n",
    "        l =  self.model.evaluate(self.featurizer.X_train, self.featurizer.y_train, batch_size=16)\n",
    "        self.result.train_loss = l[0]\n",
    "        self.result.train_acc = l[1]\n",
    "        self.result.train_sample_count = self.featurizer.X_train.shape[0]\n",
    "        self.result.class_no = self.featurizer.y_vec.shape[1]\n",
    "        \n",
    "    def evaluate_test(self):\n",
    "        l =  self.model.evaluate(self.featurizer.X_test, self.featurizer.y_test, batch_size=16)\n",
    "        self.result.test_loss = l[0]\n",
    "        self.result.test_acc = l[1]\n",
    "        self.result.test_sample_count = self.featurizer.X_test.shape[0]\n",
    "        \n",
    "    def run_and_save(self):\n",
    "        try:\n",
    "            self.fit_train()\n",
    "            self.evaluate_train()\n",
    "            self.evaluate_test()\n",
    "        except Exception as e:\n",
    "            raise\n",
    "            self.result.success = False\n",
    "            self.result.exception = type(e).__name__\n",
    "        else:\n",
    "            self.result.success = True\n",
    "        if self.config.glob_conf['nolog'] is False:\n",
    "            self.save_results()\n",
    "        \n",
    "    def save_results(self):\n",
    "        d = {}\n",
    "        d.update(self.config.to_dict())\n",
    "        d.update(self.result.to_dict())\n",
    "        Experiment.save_to_dataframe(d, Experiment.df_path)\n",
    "        \n",
    "    @staticmethod\n",
    "    def save_to_dataframe(data, fn):\n",
    "        if os.path.exists(fn):\n",
    "            df = pd.read_table(fn)\n",
    "        else:\n",
    "            df = pd.DataFrame(columns=data.keys())\n",
    "            \n",
    "        new_cols = set(data.keys()) - set(df.columns)\n",
    "        for c in new_cols:\n",
    "            df[c] = None\n",
    "        df = df.append(data, ignore_index=True)\n",
    "        df.to_csv(fn, sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "READ 1481402 lines\n",
      "5888/6000 [============================>.] - ETA: 0sSUCCESS: Train accuracy: 0.9593055555555555\n",
      "Test accuracy: 0.9449166666666666\n",
      "CPU times: user 1min 50s, sys: 22.9 s, total: 2min 13s\n",
      "Wall time: 57.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "best_cfg = {\n",
    "    'global': {\n",
    "        'nolog': True,\n",
    "    },\n",
    "    'featurizer': {\n",
    "        'last_char': 6,\n",
    "        'N': 2,\n",
    "        'use_padding': True,\n",
    "        'sample_per_class': 30000,\n",
    "        'include_smaller_ngrams': True,\n",
    "        'tag_filter': (\"NOUN\", \"VERB\"),\n",
    "        'uniq_lines': True,\n",
    "        'data_path': '/mnt/store/hlt/Language/Hungarian/Crawl/Web2/ana/xaa.tagged',\n",
    "        'max_lines': 10000000,\n",
    "    },\n",
    "    'model': {\n",
    "        'architecture': 'FFNN',\n",
    "        'layers': (20, 10),\n",
    "        'activations': ('sigmoid', 'sigmoid', 'sigmoid'),\n",
    "        'optimizer': 'rmsprop',\n",
    "        'loss': 'binary_crossentropy',\n",
    "        'metrics': ['accuracy'],\n",
    "        'nb_epoch': 300,\n",
    "        'batch_size': 500,\n",
    "        'early_stopping': True,\n",
    "    },\n",
    "}\n",
    "\n",
    "e = Experiment(best_cfg)\n",
    "e.run_and_save()\n",
    "print(e.result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('/tmp/judit/egyik', 'w') as f:\n",
    "    f.write('\\n'.join(sorted(e.featurizer.uniq_lines)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 17701)\n"
     ]
    }
   ],
   "source": [
    "print(e.featurizer.X_mtx.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "cfg = {\n",
    "    'global': {\n",
    "        'nolog': False,\n",
    "        #'comment': \"\",\n",
    "    },\n",
    "    'featurizer': {\n",
    "        'last_char': 6,\n",
    "        'N': 3,\n",
    "        'use_padding': True,\n",
    "        'sample_per_class': 50000,\n",
    "        'include_smaller_ngrams': True,\n",
    "        'tag_filter': (\"NOUN\", \"VERB\"),\n",
    "        'uniq_lines': True,\n",
    "        'data_path': '/mnt/store/hlt/Language/Hungarian/Crawl/Web2/ana/xaa.tagged',\n",
    "    },\n",
    "    'model': {\n",
    "        'architecture': 'FFNN',\n",
    "        'layers': (40, 40),\n",
    "        'activations': ('sigmoid', 'sigmoid', 'sigmoid'),\n",
    "        'optimizer': 'rmsprop',\n",
    "        'loss': 'binary_crossentropy',\n",
    "        'metrics': ['accuracy'],\n",
    "        'nb_epoch': 300,\n",
    "        'batch_size': 500,\n",
    "        'early_stopping': True,\n",
    "    },\n",
    "}\n",
    "\n",
    "cfg['model']['early_stopping'] = True\n",
    "cfg['model']['nb_epoch'] = 300\n",
    "cfg['featurizer']['sample_per_class'] = 30000\n",
    "cfg['featurizer']['N'] = 2\n",
    "cfg['featurizer']['last_char'] = 6\n",
    "for l1 in range(10, 101, 10):\n",
    "    for l2 in range(10, 101, 10):\n",
    "        print(l1, l2)\n",
    "        cfg['model']['layers'] = (l1, l2)\n",
    "        e = Experiment(cfg)\n",
    "        print(e.featurizer.X_mtx.shape)\n",
    "        e.run_and_save()\n",
    "        print(e.result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "cfg = {\n",
    "    'global': {\n",
    "        'nolog': False,\n",
    "        #'comment': \"\",\n",
    "    },\n",
    "    'featurizer': {\n",
    "        'last_char': 6,\n",
    "        'N': 2,\n",
    "        'use_padding': True,\n",
    "        'sample_per_class': 50000,\n",
    "        'include_smaller_ngrams': True,\n",
    "        'tag_filter': (\"NOUN\", \"VERB\"),\n",
    "        'uniq_lines': True,\n",
    "        'data_path': '/mnt/store/hlt/Language/Hungarian/Crawl/Web2/ana/xaa.tagged',\n",
    "    },\n",
    "    'model': {\n",
    "        'architecture': 'FFNN',\n",
    "        'layers': (40, 40),\n",
    "        'activations': ('sigmoid', 'sigmoid', 'sigmoid'),\n",
    "        'optimizer': 'rmsprop',\n",
    "        'loss': 'binary_crossentropy',\n",
    "        'metrics': ['accuracy'],\n",
    "        'nb_epoch': 300,\n",
    "        'batch_size': 500,\n",
    "        'early_stopping': True,\n",
    "    },\n",
    "}\n",
    "\n",
    "cfg['model']['early_stopping'] = True\n",
    "cfg['model']['nb_epoch'] = 300\n",
    "cfg['featurizer']['sample_per_class'] = 30000\n",
    "cfg['featurizer']['N'] = 2 \n",
    "cfg['featurizer']['last_char'] = 6\n",
    "for l1 in range(10, 101, 10):\n",
    "    for l2 in range(10, 101, 10):\n",
    "        cfg['model']['layers'] = (l1, l2)\n",
    "        e = Experiment(cfg)\n",
    "        print(e.featurizer.X_mtx.shape)\n",
    "        e.run_and_save()\n",
    "        print(e.result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "cfg = {\n",
    "    'global': {\n",
    "        'nolog': False,\n",
    "        #'comment': \"\",\n",
    "    },\n",
    "    'featurizer': {\n",
    "        'last_char': 8,\n",
    "        'N': 2,\n",
    "        'use_padding': True,\n",
    "        'sample_per_class': 50000,\n",
    "        'include_smaller_ngrams': True,\n",
    "        'tag_filter': (\"NOUN\", \"VERB\"),\n",
    "        'uniq_lines': True,\n",
    "        'data_path': '/mnt/store/hlt/Language/Hungarian/Crawl/Web2/ana/xaa.tagged',\n",
    "        'early_stopping': True,\n",
    "    },\n",
    "    'model': {\n",
    "        'architecture': 'FFNN',\n",
    "        'layers': (40, 40),\n",
    "        'activations': ('sigmoid', 'sigmoid', 'sigmoid'),\n",
    "        'optimizer': 'rmsprop',\n",
    "        'loss': 'binary_crossentropy',\n",
    "        'metrics': ['accuracy'],\n",
    "        'nb_epoch': 300,\n",
    "        'batch_size': 500,\n",
    "        'early_stopping': True,\n",
    "    },\n",
    "}\n",
    "#e = Experimenterimene.featurizere.featurizer.X_mtx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"DONEEE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "cfg = {\n",
    "    'global': {\n",
    "        'nolog': False,\n",
    "    },\n",
    "    'featurizer': {\n",
    "        'last_char': 6,\n",
    "        'N': 2,\n",
    "        'use_padding': True,\n",
    "        'sample_per_class': 20,\n",
    "        'include_smaller_ngrams': True,\n",
    "        #'tag_filter': (\"NOUN\", \"VERB\", \"ADJ\"),\n",
    "        'tag_filter': (\"VERB\", ),\n",
    "        'grep_filter': (\"<PERS<1\", \"<PERS<2\" ),\n",
    "        'max_lines': 20000000,\n",
    "        'uniq_lines': True,\n",
    "        'early_stopping': True,\n",
    "    },\n",
    "    'model': {\n",
    "        'architecture': 'FFNN',\n",
    "        'layers': (40, 40),\n",
    "        'activations': ('sigmoid', 'sigmoid', 'sigmoid'),\n",
    "        'optimizer': 'rmsprop',\n",
    "        'loss': 'categorical_crossentropy',\n",
    "        'metrics': ['accuracy'],\n",
    "        'nb_epoch': 300,\n",
    "        'batch_size': 500,\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "general_filters = [\n",
    "    (\"<PLUR\", ),\n",
    "    (\"<ACC\", ),\n",
    "]\n",
    "verb_filters = [\n",
    "    (\"<PERS<1\", \"<PERS<2\"),\n",
    "    (\"<COND\", ),\n",
    "]\n",
    "noun_filters = [\n",
    "    (\"<PLUR\", ),\n",
    "    (\"<CAS<ACC\", \"<CAS<DAT\", \"<CAS<INE\", \"<CAS<INS\", \"<CAS<SBL\"),\n",
    "]\n",
    "pos_filters = [\n",
    "    (\"ADV\", \"ADJ\"),\n",
    "]\n",
    "\n",
    "N_range = [1, 2]\n",
    "lc_range = [5, 6, 7]\n",
    "sample_size = 30000\n",
    "\n",
    "def run_filter_with_params(cfg, filters, N_range, lc_range, typ='grep_filter'):\n",
    "    for filt in filters:\n",
    "        if typ == 'grep_filter':\n",
    "            class_n = len(filt) + 1\n",
    "        elif typ == 'tag_filter':\n",
    "            class_n = len(filt)\n",
    "        cfg['featurizer'][typ] = filt\n",
    "        cfg['featurizer']['sample_per_class'] = sample_size / class_n\n",
    "        if class_n == 2:\n",
    "            cfg['model']['loss'] = 'binary_crossentropy'\n",
    "        elif class_n > 2:\n",
    "            cfg['model']['loss'] = 'categorical_crossentropy'\n",
    "        for N in N_range:\n",
    "            cfg['featurizer']['N'] = N\n",
    "            for last_char in lc_range:\n",
    "                cfg['featurizer']['last_char'] = last_char\n",
    "                e = Experiment(cfg)\n",
    "                print(\"Starting experiment with filter [{0}], last_char [{1}], N [{2}], classes [{3}], size: [{4}]\".format(\n",
    "                        filt, last_char, N, class_n, e.featurizer.X_mtx.shape))\n",
    "                e.run_and_save()\n",
    "    \n",
    "    \n",
    "done = False\n",
    "if done is False:\n",
    "    print(\"GENERAL filters\")\n",
    "    cfg['featurizer']['tag_filter'] = None\n",
    "    run_filter_with_params(cfg, general_filters, N_range, lc_range)\n",
    "    print(\"DONE\")\n",
    "\n",
    "    print(\"VERB filters\")\n",
    "    cfg['featurizer']['tag_filter'] = (\"VERB\", )\n",
    "    run_filter_with_params(cfg, verb_filters, N_range, lc_range)\n",
    "\n",
    "    print(\"NOUN filters\")\n",
    "    cfg['featurizer']['tag_filter'] = (\"NOUN\", )\n",
    "    run_filter_with_params(cfg, noun_filters, N_range, lc_range)\n",
    "    print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "verb_filters = [\n",
    "    (\"<PAST\", ),\n",
    "]\n",
    "noun_filters = [\n",
    "    (\"<CAS<ACC\", \"<CAS<DAT\", \"<CAS<INE\", \"<CAS<INS\", \"<CAS<SBL\", \"<CAS<SUE\", \"<CAS<ALL\",\n",
    "     \"<CAS<ILL\", \"<CAS<ELA\", \"<CAS<DEL\"),\n",
    "]\n",
    "pos = [\n",
    "    (\"CONJ\", \"NOUN\"),\n",
    "    (\"CONJ\", \"NOUN\", \"ADJ\", \"VERB\"),\n",
    "]\n",
    "N_range = [1, 2]\n",
    "lc_range = [2, 3, 4, 5, 6, 7, 8]\n",
    "sample_size = 60000\n",
    "\n",
    "cfg['global']['nolog'] = False\n",
    "print(\"POS filter\")\n",
    "cfg['featurizer']['grep_filter'] = None\n",
    "run_filter_with_params(cfg, pos, N_range, lc_range, 'tag_filter')\n",
    "\n",
    "print(\"VERB filters\")\n",
    "cfg['featurizer']['tag_filter'] = (\"VERB\", )\n",
    "run_filter_with_params(cfg, verb_filters, N_range, lc_range)\n",
    "\n",
    "print(\"NOUN filters\")\n",
    "cfg['featurizer']['tag_filter'] = (\"NOUN\", )\n",
    "run_filter_with_params(cfg, noun_filters, N_range, lc_range)\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "print(\"ADV vs ADJ\")\n",
    "N_range = [3]\n",
    "lc_range = [5, 6, 7]\n",
    "sample_size = 60000\n",
    "\n",
    "cfg['featurizer']['grep_filter'] = None\n",
    "cfg['featurizer']['sample_per_class'] = 30000\n",
    "cfg['model']['loss'] = 'binary_crossentropy'\n",
    "cfg['featurizer']['tag_filter'] = (\"ADV\", \"ADJ\")\n",
    "for N in N_range:\n",
    "    cfg['featurizer']['N'] = N\n",
    "    for last_char in lc_range:\n",
    "        cfg['featurizer']['last_char'] = N \n",
    "        e = Experiment(cfg)\n",
    "        e.run_and_save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "cfg = {\n",
    "    'global': {\n",
    "        'nolog': False,\n",
    "        'comment': \"limit freq[kr_tag][replace_to] to 5. A single KR tag may have been replaced at most 5 times.\",\n",
    "    },\n",
    "    'featurizer': {\n",
    "        'data_path': '/mnt/store/judit/projects/ulm/vitmav45-2016-MorphoDeep/dat/webcorp_kr_clustered.limit_repl',\n",
    "        'last_char': 6,\n",
    "        'N': 2,\n",
    "        'use_padding': True,\n",
    "        'sample_per_class': 10,\n",
    "        'include_smaller_ngrams': True,\n",
    "        #'tag_filter': (\"NOUN\", \"VERB\", \"ADJ\"),\n",
    "        'tag_filter': None,\n",
    "        'grep_filter': None,\n",
    "        'max_lines': 20000000,\n",
    "    },\n",
    "    'model': {\n",
    "        'architecture': 'FFNN',\n",
    "        'layers': (40, 40),\n",
    "        'activations': ('sigmoid', 'sigmoid', 'sigmoid'),\n",
    "        'optimizer': 'rmsprop',\n",
    "        'loss': 'categorical_crossentropy',\n",
    "        'metrics': ['accuracy'],\n",
    "        'nb_epoch': 300,\n",
    "        'batch_size': 500,\n",
    "    },\n",
    "}\n",
    "sample_range = [500]\n",
    "N_range = [1, 2]\n",
    "lc_range = [5, 6]\n",
    "\n",
    "repl_limit = [30]\n",
    "\n",
    "for r in repl_limit:\n",
    "    cfg['featurizer']['data_path'] = \"{0}{1}\".format(cfg['featurizer']['data_path'], r)\n",
    "    cfg['featurizer']['tag_limit_in_cluster'] = r\n",
    "    for s in sample_range:\n",
    "        print(r, s)\n",
    "        cfg['featurizer']['sample_per_class'] = s\n",
    "        for N in N_range:\n",
    "            cfg['featurizer']['N'] = N\n",
    "            for lc in lc_range:\n",
    "                cfg['featurizer']['last_char'] = lc\n",
    "                e = Experiment(cfg)\n",
    "                e.run_and_save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "cfg = {\n",
    "    'global': {\n",
    "        'nolog': False,\n",
    "    },\n",
    "    'featurizer': {\n",
    "        'data_path': '/mnt/store/judit/projects/ulm/vitmav45-2016-MorphoDeep/dat/webcorp_kr_clustered.limit_repl',\n",
    "        'last_char': 6,\n",
    "        'N': 2,\n",
    "        'use_padding': True,\n",
    "        'sample_per_class': 300,\n",
    "        'include_smaller_ngrams': True,\n",
    "        #'tag_filter': (\"NOUN\", \"VERB\", \"ADJ\"),\n",
    "        'tag_filter': None,\n",
    "        'grep_filter': None,\n",
    "        'max_lines': 20000000,\n",
    "        'uniq_lines': True,\n",
    "    },\n",
    "    'model': {\n",
    "        'architecture': 'FFNN',\n",
    "        'layers': (40, 40),\n",
    "        'activations': ('sigmoid', 'sigmoid', 'sigmoid'),\n",
    "        'optimizer': 'rmsprop',\n",
    "        'loss': 'categorical_crossentropy',\n",
    "        'metrics': ['accuracy'],\n",
    "        'nb_epoch': 300,\n",
    "        'batch_size': 500,\n",
    "    },\n",
    "}\n",
    "sample_range = [100]\n",
    "N_range = [2]\n",
    "lc_range = [6]\n",
    "\n",
    "repl_limit = [10, 25, 50, 75, 100, 200, 500, 1000, 2000, 5000, 10000, 50000]\n",
    "repl_limit = [2, 3, 5, 15, 20]\n",
    "repl_limit += [1, 4, 6, 7, 8, 9, 11, 12, 13]\n",
    "#repl_limit = [14]\n",
    "data_path = cfg['featurizer']['data_path']\n",
    "cfg['featurizer']['uniq_lines'] = True\n",
    "\n",
    "for r in repl_limit:\n",
    "    fn = \"{0}{1}\".format(data_path, r)\n",
    "    if not os.path.exists(fn):\n",
    "        print(\"FILE NOT FOUND: {}\".format(fn))\n",
    "        continue\n",
    "    cfg['featurizer']['data_path'] = fn\n",
    "    cfg['featurizer']['tag_limit_in_cluster'] = r\n",
    "    cfg['featurizer']['shuffled'] = True\n",
    "    cfg['global']['comment'] = \"limit freq[kr_tag][replace_to] to {0}. A single KR tag may have been replaced at most \" \\\n",
    "        \"{0} times.\".format(r)\n",
    "    for s in sample_range:\n",
    "        cfg['featurizer']['sample_per_class'] = s\n",
    "        for N in N_range:\n",
    "            cfg['featurizer']['N'] = N\n",
    "            for lc in lc_range:\n",
    "                print(r, s, N, lc)\n",
    "                cfg['featurizer']['last_char'] = lc\n",
    "                e = Experiment(cfg)\n",
    "                e.run_and_save()\n",
    "                print(e.result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"DONEEEE\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
